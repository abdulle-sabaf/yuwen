{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from outils import read, keys, load_cn_json, dump_cn_json, 中转数, 数转中, set_char_colors, nice_print, sort_dict_with, dump_cn_json_compact\n",
    "\n",
    "def wrap(s, wrapper=\"{}\", keep_wrapper=False):\n",
    "    if s:\n",
    "        return wrapper[0] + s + wrapper[-1]\n",
    "    if keep_wrapper:\n",
    "        return wrapper\n",
    "    return \"\"\n",
    "\n",
    "def make_params(params, wrapper=\"[]\", sep=\",\"):\n",
    "    return wrap(sep.join(params), wrapper)\n",
    "\n",
    "def wrap_env(name, content, params=[], param_wrapper=\"[]\", param_sep=\",\"):\n",
    "    out = r\"\\begin\" + wrap(name) + make_params(params, wrapper=param_wrapper, sep=param_sep) + \"\\n\"\n",
    "    lines = content[:-1].split(\"\\n\")  # presume content ends with \\n\n",
    "    for line in lines:\n",
    "        out += \"    \" + line + \"\\n\"\n",
    "    out += r\"\\end\" + wrap(name) + \"\\n\"\n",
    "    return out\n",
    "\n",
    "def wrap_method(method, content=\"\", wrapper=\"{}\", keep_wrapper=True, params=[], param_wrapper=\"[]\", param_sep=\",\"):\n",
    "    return '\\\\' + method + make_params(params, wrapper=param_wrapper, sep=param_sep) + wrap(content, wrapper=wrapper, keep_wrapper=keep_wrapper)\n",
    "\n",
    "def zihao(n):\n",
    "    return wrap_method(\"zihao\", str(n))\n",
    "\n",
    "def package_update_xcolor(packages, texts):\n",
    "    xcolor = packages[\"xcolor\"]\n",
    "    xcolor[\"defined_colors\"] = {}\n",
    "    for _, text in texts.items():\n",
    "        if \"character_colors\" in text:\n",
    "            for key, val in text[\"character_colors\"].items():\n",
    "                xcolor[\"defined_colors\"][key] = val\n",
    "    packages[\"xcolor\"] = xcolor\n",
    "\n",
    "def make_ctex_env(document_class=\"ctexbook\", document_class_params=(\"12pt\", \"UTF-8\",\"openany\"), packages={\"ctex\": [], \"titlesec\": []}, mainfont=\"Mona Sans Light\", lineskip=\"4pt\", parskip=\"10pt\", title=\"标题\", author=\"\", date=False, toc=True):\n",
    "    \"\"\"make header and footer for ctexbook environment. \n",
    "    header\n",
    "    1. documentclass and parameters \n",
    "    2. packages\n",
    "    3. geometry and fonts\n",
    "    4. package setups\n",
    "    5. global typesettings\n",
    "    6. begin document\n",
    "    footer\n",
    "    1. end document\n",
    "    \"\"\"\n",
    "    # ## header ##\n",
    "\n",
    "    # document class\n",
    "    header = r\"\\documentclass\"+ make_params(document_class_params) + wrap(document_class) + \"\\n\"\n",
    "    \n",
    "    # packages\n",
    "    packages_str = \"\"\n",
    "    for name in packages:\n",
    "        # print(package)\n",
    "        package_declarations = \"\"\n",
    "        if \"declarations\" in packages[name]:\n",
    "            package_declarations = make_params(packages[name]['declarations'])\n",
    "        packages_str += r\"\\usepackage\" + package_declarations + wrap(name) + \"\\n\"\n",
    "    # print(packages_str)\n",
    "    header += packages_str + \"\\n\"\n",
    "\n",
    "    # geometry <-- geometry package\n",
    "    if \"geometry\" in packages:\n",
    "        geometry = packages[\"geometry\"]\n",
    "        paper_type = geometry[\"paper_size\"]\n",
    "        paddings = geometry[\"paddings\"]\n",
    "        left = paddings[\"left\"]\n",
    "        right = paddings[\"right\"]\n",
    "        top = paddings[\"top\"]\n",
    "        bottom = paddings[\"bottom\"]\n",
    "        header += wrap_method(\"geometry\", f\"{paper_type}paper,left={left},right={right},top={top},bottom={bottom}\") + \"\\n\"\n",
    "    \n",
    "    # fonts\n",
    "    header += r\"\\renewcommand{\\footnotesize}{\\fontsize{8.5pt}{10.5pt}\\selectfont}\" + \"\\n\"\n",
    "    header += wrap_method(\"setmainfont\", mainfont) + \"\\n\"\n",
    "    header += r\"\\setCJKmainfont[BoldFont=STZhongsong]{汉字之美仿宋GBK 免费}\" + \"\\n\"\n",
    "    header += r\"\\xeCJKDeclareCharClass{CJK}{`0 -> `9}\" + \"\\n\"  # apply CJK font to numbers\n",
    "    header += r\"\\xeCJKsetup{AllowBreakBetweenPuncts=true}\" + \"\\n\"  # line alignment\n",
    "\n",
    "    if \"footmisc\" in packages:\n",
    "        footnote_settings_content = \"\".join([r\"{\\ding{\"+str(192+i)+r\"}}\" for i in range(10)])\n",
    "        footnote_settings = wrap_method(\"DefineFNsymbols\", footnote_settings_content, params=[\"circled\"], param_wrapper=\"{}\")\n",
    "        header += footnote_settings + \"\\n\"\n",
    "        header += wrap_method(\"setfnsymbol\", \"circled\") + \"\\n\"\n",
    "\n",
    "    # package setups\n",
    "    # xpinyin\n",
    "    if \"xpinyin\" in packages:\n",
    "        pyr = packages['xpinyin']['ratio']  # size ratio\n",
    "        vsep = packages['xpinyin']['vsep']  # vertical gap\n",
    "        vsep_str = \"vsep={\" + vsep + \"}\"\n",
    "        hsep = packages['xpinyin']['hsep']  # horizontal gap\n",
    "        hsep_str = \"hsep={\" + f\"{hsep} plus {hsep}\" + \"}\"\n",
    "        header += wrap_method(\"xpinyinsetup\", f\"ratio={pyr},{hsep_str},{vsep_str}\") + \"\\n\"  # pinyin settings\n",
    "\n",
    "    # hanzibox\n",
    "    if \"hanzibox\" in packages:\n",
    "        hanzibox = packages[\"hanzibox\"]\n",
    "        frametype = hanzibox['frametype']\n",
    "        framelinewidth = hanzibox['framelinewidth']\n",
    "        width = hanzibox['width']\n",
    "        resize = hanzibox['resize']\n",
    "        framecolor = hanzibox[\"framecolor\"]\n",
    "        pinyinline = hanzibox['pinyinline']\n",
    "        pinyinf = hanzibox['pinyinf']\n",
    "        pinyincolor = hanzibox['pinyincolor']\n",
    "        charcolor = hanzibox['charcolor']\n",
    "        charf = \"charf={\" + hanzibox[\"charf\"][\"font\"] + hanzibox[\"charf\"][\"fontsize\"] + \"}\"\n",
    "        header += wrap_method(\"hanziboxset\", f\"frametype={frametype},framelinewidth={framelinewidth},width={width},resize={resize},pinyinline={pinyinline},framecolor={framecolor},{charf},pinyinf={pinyinf},pinyincolor={pinyincolor},charcolor={charcolor}\") + \"\\n\"  # hanzibox settings\n",
    "\n",
    "    # package setups\n",
    "    # xcolor\n",
    "    if \"xcolor\" in packages:\n",
    "        defcolor_str = \"\"\n",
    "        for key, (r, g, b) in packages[\"xcolor\"][\"defined_colors\"].items():\n",
    "            rgb_plate = f\"{r},{g},{b}\"\n",
    "            defcolor_str += wrap_method(\"definecolor\", key) + r\"{RGB}{\" + rgb_plate + r\"}\" + \"\\n\"\n",
    "        header += defcolor_str + \"\\n\"\n",
    "\n",
    "    # global typesettings\n",
    "    # title format\n",
    "    header += r\"\\titleformat{\\chapter}{\\zihao{-1}\\bfseries}{ }{16pt}{}\" + \"\\n\"\n",
    "    header += r\"\\titleformat{\\section}{\\zihao{-2}\\bfseries}{ }{0pt}{}\" + \"\\n\"\n",
    "    header += r\"\\title\" + wrap(r\"\\zihao{0} \\bfseries \" + title) + \"\\n\"\n",
    "    # line and paragraph skips\n",
    "    header += r\"\\setlength{\\lineskip}{\" + lineskip + \"}\\n\"  # skip length after line\n",
    "    header += r\"\\setlength{\\parskip}{\" + parskip + \"}\\n\"  # extra skip for paragraphs \n",
    "    # front page format\n",
    "    if author:  # author format\n",
    "        header += r\"\\author{\\zihao{2} \\texttt\" + wrap(author) + \"}\\n\"\n",
    "    else:\n",
    "        header += r\"\\author{}\" + \"\\n\"\n",
    "    if date:  # date format\n",
    "        header += r\"\\date{\\bfseries\\today}\" + \"\\n\"\n",
    "    else:\n",
    "        header += r\"\\date{}\" + \"\\n\"\n",
    "    \n",
    "    # begin document\n",
    "    header += r\"\\begin\" + wrap(\"document\") + \"\\n\"\n",
    "    header += r\"\\maketitle\" + \"\\n\"\n",
    "    if toc:\n",
    "        header += r\"\\tableofcontents\" + \"\\n\"\n",
    "    header += r\"\\newpage\" + \"\\n\"\n",
    "    \n",
    "    # ## footer ##\n",
    "\n",
    "    # end document\n",
    "    footer = r\"\\end\" + wrap(\"document\") + \"\\n\"\n",
    "    return header, footer\n",
    "\n",
    "def read_text(path, format=\"散文\"):\n",
    "    \"\"\"Read raw text and formalize to json\n",
    "    Inputs: \n",
    "    path (str): file path to the raw text.\n",
    "    format (str): format of the text.\n",
    "    Output:\n",
    "    out (dict): a jsonifiable dictionary with formalized text.\n",
    "    Example:\n",
    "    out[\"format\"]     : format of the text (in the sense of tex printing).\n",
    "    out[\"genre\"]      : genre and other tags of the text.\n",
    "    out[\"content\"]    : content of the text. A list of strings.\n",
    "    out[\"grade\"]      : recommanded student grade (for the purpose of eduation).\n",
    "    out[\"title\"]      : title of the text.\n",
    "    out[\"author\"]     : author of the text.\n",
    "    out[\"remarks\"]    : remarks concerning the text.\n",
    "    out[\"footnotes\"]  : footnotes of the content of the text.\n",
    "    out[\"endnotes\"]   : endnotes of the content of the text.\n",
    "    out[\"vocabulary\"] : vocabulary to learn (for the purpose of eduation).\n",
    "    \"\"\"\n",
    "    lines = read(path)\n",
    "    out = {}\n",
    "    title = \"\"\n",
    "    if len(lines) and len(lines[0]):\n",
    "        author = \"\"\n",
    "        grade = 0\n",
    "        footnotes = []\n",
    "        endnotes = []\n",
    "        vocabulary = []\n",
    "        remarks = []\n",
    "        content = []\n",
    "        out[\"format\"] = format\n",
    "        out[\"genre\"] = [format]\n",
    "        # return lines\n",
    "        if format in (\"散文\", \"书信\", \"小说\", \"剧本\"):\n",
    "            for line in lines:\n",
    "                line0 = line.strip()\n",
    "                if line0:\n",
    "                    if not title:\n",
    "                        title = line0\n",
    "                    elif grade < 1 and line.startswith(\"年级：\"):\n",
    "                        grade = int(line0[3:])\n",
    "                    elif not author and line.startswith(\"作者：\"):\n",
    "                        author = line0[3:]\n",
    "                    elif line.startswith(\"备注：\"):\n",
    "                        remarks.append(line0[3:])\n",
    "                    elif line.startswith(\"注释：\"):\n",
    "                        footnotes.append(line0[3:])\n",
    "                    elif line.startswith(\"脚注：\"):\n",
    "                        footnotes.append(line0[3:])\n",
    "                    elif line.startswith(\"尾注：\"):\n",
    "                        endnotes.append(line0[3:])\n",
    "                    elif line.startswith(\"词汇：\"):\n",
    "                        vocabulary.extend(line0[3:].split())\n",
    "                    else:\n",
    "                        content.append(line0)\n",
    "        elif format == \"诗歌\":\n",
    "            para = []\n",
    "            for line in lines:\n",
    "                line0 = line.strip()\n",
    "                if line0:\n",
    "                    if not title:\n",
    "                        title = line0\n",
    "                    elif grade < 1 and line.startswith(\"年级：\"):\n",
    "                        grade = int(line0[3:])\n",
    "                    elif not author and line.startswith(\"作者：\"):\n",
    "                        author = line0[3:]\n",
    "                    elif line.startswith(\"备注：\"):\n",
    "                        remarks.append(line0[3:])\n",
    "                    elif line.startswith(\"注释：\"):\n",
    "                        footnotes.append(line0[3:])\n",
    "                    elif line.startswith(\"脚注：\"):\n",
    "                        footnotes.append(line0[3:])\n",
    "                    elif line.startswith(\"尾注：\"):\n",
    "                        endnotes.append(line0[3:])\n",
    "                    elif line.startswith(\"词汇：\"):\n",
    "                        vocabulary.extend(line0[3:].split())\n",
    "                    else:\n",
    "                        para.append(line0)\n",
    "                elif len(para):\n",
    "                    content.append(\"|#|\".join(para))\n",
    "                    para = []\n",
    "            if len(para):\n",
    "                content.append(\"|#|\".join(para))\n",
    "        # make footnotes dict\n",
    "        footdict = {}\n",
    "        i = 0\n",
    "        keybase = \"fn\"\n",
    "        content = \"@\".join(content)\n",
    "        for note in footnotes:\n",
    "            word = \"\"\n",
    "            if note.startswith(\"〔\"):\n",
    "                word = note.split(\"〕\")[0][1:]\n",
    "                key = keybase + str(i+1)\n",
    "                footdict[key] = note\n",
    "                i += 1\n",
    "            elif \"〕\" in note:  # key is already marked in the text with the format \"\\apost{a...}\".\n",
    "                key = note.split(\"〕\")[0].split(\"〔\")[0]\n",
    "                footdict[key] = \"\".join(note.split(key)[1:])\n",
    "            # print(word)\n",
    "            if word:  # find the position to insert footnote and mark\n",
    "                nfin = content.find(word) + len(word)\n",
    "                content = content[:nfin] + r\"\\apost{\" + key + \"}\" + content[nfin:]\n",
    "        if \"|#|\" in content:\n",
    "            content_new = []\n",
    "            for para in content.split(\"@\"):\n",
    "                content_new.append(para.split(\"|#|\"))\n",
    "            content = content_new\n",
    "        else:\n",
    "            content = content.split(\"@\")\n",
    "        \n",
    "        out[\"title\"] = title\n",
    "        out[\"author\"] = author\n",
    "        out[\"content\"] = content\n",
    "        out[\"remarks\"] = remarks\n",
    "        out[\"footnotes\"] = footdict\n",
    "        out[\"endnotes\"] = endnotes\n",
    "        out[\"vocabulary\"] = vocabulary\n",
    "        if grade:\n",
    "            out[\"grade\"] = grade\n",
    "    return title, out\n",
    "\n",
    "def text_content_to_tex_str(text, verbose=0, verseprop=0.5, format=\"散文\", footnotes={}, endnotes=[]):\n",
    "    \"\"\"convert the content of a text to text string ready for tex.\n",
    "    the format varies by genre:\n",
    "    散文、小说\n",
    "    书信\n",
    "    诗歌\n",
    "    剧本\n",
    "    \"\"\" \n",
    "    content = text[\"content\"]\n",
    "    if \"footnotes\" in text:\n",
    "        footnotes = text[\"footnotes\"]\n",
    "    if \"format\" in text:\n",
    "        format = text[\"format\"]\n",
    "    out = \"\"\n",
    "    if format in (\"散文\", \"小说\",):\n",
    "        out = \"\\n\\n\".join(content) + \"\\n\"\n",
    "    elif  format == \"书信\":\n",
    "        if verbose and not content[0].endswith(\"：\"):\n",
    "            print(\"错误：第一行不是抬头\")\n",
    "            return \"格式错误\\n\"\n",
    "        out = r\"\\noindent \" + content[0] + \"\\n\\n\" + wrap_method(\"vspace\", \"24pt\") + \"\\n\\n\"\n",
    "        toright = False\n",
    "        toright_content = \"\"\n",
    "        for line in content[1:]:\n",
    "            if line:\n",
    "                if toright:\n",
    "                    toright_content += line + \"\\n\\n\"\n",
    "                else:\n",
    "                    out += line + \"\\n\\n\"\n",
    "            else:\n",
    "                toright = True\n",
    "        out += wrap_method(\"vspace\", \"36pt\") + \"\\n\\n\"\n",
    "        out += wrap_env(\"flushright\", toright_content) + \"\\n\\n\"\n",
    "    elif  format == \"诗歌\":\n",
    "        if not isinstance(content[0], list):\n",
    "            content = [content]\n",
    "        \n",
    "        lineskip = \" \\\\\\\\\\n\"\n",
    "        # parskip = \"\\n\" + wrap_method(\"vspace\", \"4pt\") + \"\\n\\n\"\n",
    "        parskip = \"\\n\\n\"\n",
    "        out = parskip.join([wrap_env(\"verse\", lineskip.join(par) + \"\\n\", params=[str(verseprop)+\"\\\\linewidth\"]) for par in content])\n",
    "    elif format == \"剧本\":\n",
    "        name_set = text[\"characters\"]\n",
    "        for line in content:\n",
    "            if line.startswith(\"\\\\item[\"):\n",
    "                name = line.split(\"]\")[0][6:]\n",
    "                colored_name = r\"{\\color{\" + name_set[name] + r\"} \" + name + r\"}\"\n",
    "                out += \"\\\\item[\" + colored_name + \"]\" + \"]\".join(line.split(\"]\")[1:])\n",
    "            elif line.startswith(\"$\"):\n",
    "                colored_line = line\n",
    "                for name in name_set:\n",
    "                    colored_line = colored_line.replace(name, r\"{\\color{\" + name_set[name] + r\"} \" + name + r\"}\")\n",
    "                out += colored_line\n",
    "            else:\n",
    "                out += line\n",
    "            out += \"\\n\\n\"\n",
    "    for key in footnotes:\n",
    "        out = out.replace(\"apost{\"+key+\"}\", \"footnote{\" + footnotes[key] + \"}\")\n",
    "    return out\n",
    "\n",
    "def endnotes_to_str(endnotes, verbose=0, pinyin=False):\n",
    "    \"\"\"convert the endnotes to text string ready for tex.\"\"\"\n",
    "    out = \"\"\n",
    "    notes = \"\"\n",
    "    for note in endnotes:\n",
    "        if pinyin and note.startswith(\"〔\"):  # add pinyin\n",
    "            suite = note[1:].split(\"〕\")\n",
    "            notes += r\"\\item \" + note[0] + r\"\\xpinyin*{\" + suite[0] + r\"}〕\" + r\"〕\".join(suite[1:]) + \"\\n\"\n",
    "        else:\n",
    "            notes += r\"\\item \" + note + \"\\n\"\n",
    "    if notes:\n",
    "        out = r\"\\newpage\" + \"\\n\\n\" + r\"\\textbf{注释}：\" + \"\\n\\n\" + r\"\\vspace{-1em}\" + \"\\n\\n\"\n",
    "        out += wrap_env(\"itemize\", r\"\\setlength\\itemsep{-0.2em}\" + \"\\n\" + notes)\n",
    "    return out\n",
    "\n",
    "def shizi_to_str(zis, n=8):\n",
    "    out = r\"\\clearpage\" + \"\\n\\n\"\n",
    "    boxes = \"\"\n",
    "    i = 0\n",
    "    for zi in zis:\n",
    "        boxes += wrap_method(\"hanzibox\", zi)\n",
    "        i += 1\n",
    "        if i == n:\n",
    "            boxes += \"\\n\\n\"\n",
    "            i = 0\n",
    "    out += wrap_env(\"center\", boxes + \"\\n\\n\")\n",
    "    return out\n",
    "\n",
    "def xiezi_to_str(zis, ncol=2, nex=4, hspace=1):\n",
    "    out = \"\"\n",
    "    boxes = \"\"\n",
    "    i = 0\n",
    "    for zi in zis:\n",
    "        boxes += wrap_method(\"hanzibox\", zi)\n",
    "        for j in range(nex):\n",
    "            boxes += wrap_method(\"hanzibox\", \"\")\n",
    "        i += 1\n",
    "        if i == ncol:\n",
    "            boxes += \"\\n\\n\"\n",
    "            i = 0\n",
    "        else:\n",
    "            boxes += wrap_method(\"hspace\", f\"{hspace}em\")\n",
    "    out += boxes + \"\\n\\n\"\n",
    "    # out += wrap_env(\"center\", boxes + \"\\n\\n\")\n",
    "    return out\n",
    "\n",
    "def text_to_tex_str(text, typesettings={\"font\": {\"title\": {\"size\": 2}, \"plaintext\": {\"size\": \"normalsize\"}}, \"vspaces\": {\"after_title\": 12, \"after_author\": 6, \"after_content\": 6}}):\n",
    "    \"\"\"convert a text object to text string ready for tex\n",
    "    \"\"\"\n",
    "    out = \"\"\n",
    "    content = \"\"\n",
    "    # title_fontsize = typesettings[\"font\"][\"title\"][\"size\"]\n",
    "    # title = wrap_method(\"textbf\", zihao(title_fontsize) + \" \" + text[\"title\"]) + \"\\n\"\n",
    "    title = wrap_method(\"chapter\", text[\"title\"]) + \"\\n\\n\"\n",
    "    content += title\n",
    "    # content = wrap_env(\"center\", content) + \"\\n\"\n",
    "    # content += wrap_method(\"vspace\", f\"{typesettings['vspaces']['after_title']}pt\") + \"\\n\\n\"\n",
    "    content += wrap_env(typesettings[\"font\"][\"plaintext\"][\"size\"], \"\\n\" + text_content_to_tex_str(text) + \"\\n\")\n",
    "    # content += wrap_method(\"vspace\", f\"{typesettings['vspaces']['after_content']}pt\") + \"\\n\\n\"\n",
    "    out += content + \"\\n\\n\"\n",
    "    # out += wrap_method(\"newpage\", keep_wrapper=False) + \"\\n\\n\"\n",
    "    if \"endnotes\" in text:\n",
    "        out += endnotes_to_str(text[\"endnotes\"])\n",
    "    if \"shizi\" in text:\n",
    "        nchars = 10\n",
    "        if len(text[\"shizi\"]) % 10 == 1:\n",
    "            nchars = 8\n",
    "        out += shizi_to_str(text[\"shizi\"], n=nchars) + \"\\n\\n\"\n",
    "        if \"xiezi\" in text:\n",
    "            out += xiezi_to_str(text[\"xiezi\"]) + \"\\n\\n\"\n",
    "    return out\n",
    "\n",
    "def add_text(texts, title, content, format=\"散文\", tags=[]):\n",
    "    \"\"\"Add a text to the dictionary of texts.\n",
    "    Inputs:\n",
    "    texts (dict): dictionary of texts. title --> content.\n",
    "    title (str): title of the text.\n",
    "    content (dict): content of the text.\n",
    "    format (str): format of the text.\n",
    "    tags (list of str): tags to describe the text.\n",
    "    Output:\n",
    "    texts: updated dictionary of texts. \n",
    "    \"\"\"\n",
    "    if len(tags):\n",
    "        content[\"genre\"] = tags\n",
    "    if format == \"剧本\":\n",
    "        if title not in texts:\n",
    "            script_keys = []\n",
    "            for _, text in texts.items():\n",
    "                if text[\"format\"] == \"剧本\" and \"key\" in text:\n",
    "                    script_keys.append(int(text[\"key\"].split(\"-\")[1]))\n",
    "            if len(script_keys):\n",
    "                script_key = \"script-\" + str(max(script_keys) + 1)\n",
    "            else:\n",
    "                script_key = \"script-1\"\n",
    "                \n",
    "        else:\n",
    "            script_key = texts[title][\"key\"]\n",
    "        name_set, color_set = set_char_colors(content[\"content\"], script_key)\n",
    "        \n",
    "    texts[title] = content\n",
    "    if format == \"剧本\":\n",
    "        texts[title][\"key\"] = script_key\n",
    "        texts[title][\"characters\"] = name_set\n",
    "        texts[title][\"character_colors\"] = color_set\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印页面设置：纸号，页边距等\n",
    "geometry = {}\n",
    "geometry[\"paper_size\"] = \"a5\"  # 使用A5纸\n",
    "paddings = {}  # 页边距\n",
    "paddings[\"left\"] = \"1.4cm\"\n",
    "paddings[\"right\"] = \"1.4cm\"\n",
    "paddings[\"top\"] = \"2.3cm\"\n",
    "paddings[\"bottom\"] = \"2.3cm\"\n",
    "geometry[\"paddings\"] = paddings\n",
    "\n",
    "# 拼音设置： xpinyin宏包\n",
    "pinyin = {}\n",
    "pinyin[\"ratio\"] = \"0.5\"\n",
    "pinyin[\"hsep\"] = \".6em\"\n",
    "pinyin[\"vsep\"] = \"1em\"\n",
    "\n",
    "# 田字格设置：hanzibox宏包\n",
    "# \\hanziboxset{frametype=咪,framelinewidth=0.5pt,width=1.0cm,resize=real,pinyinline=true,framecolor=red,charf={\\kaishu\\huge},pinyinf=\\scriptsize,pinyincolor=green!30!black,charcolor=green!30!black}\n",
    "hanzibox = {}\n",
    "hanzibox[\"frametype\"] = \"咪\"\n",
    "hanzibox[\"framelinewidth\"] = \"0.5pt\"\n",
    "hanzibox[\"width\"] = \"0.9cm\"\n",
    "hanzibox[\"resize\"] = \"real\"\n",
    "hanzibox[\"pinyinline\"] = \"true\"\n",
    "hanzibox[\"framecolor\"] = \"red\"\n",
    "hanzibox[\"pinyinf\"] = r\"\\scriptsize\"\n",
    "hanzibox[\"charf\"] = {\"font\": r\"\\kaishu\", \"fontsize\": r\"\\huge\"}\n",
    "hanzibox[\"pinyincolor\"] = r\"green!30!black\"\n",
    "hanzibox[\"charcolor\"] = r\"green!30!black\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小学"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_xx = \"../src/小学/\"\n",
    "\n",
    "# 打印小学古诗（分层）\n",
    "packages = {}\n",
    "packages[\"ctex\"] = []\n",
    "packages[\"titlesec\"] = []\n",
    "packages[\"xeCJK\"] = []\n",
    "packages[\"fontspec,xunicode,xltxtra\"] = []\n",
    "packages[\"xpinyin\"] = pinyin\n",
    "packages[\"xpinyin\"]['ratio'] = \"0.44\"\n",
    "packages[\"xpinyin\"]['hsep'] = \".6em\"\n",
    "packages[\"geometry\"] = geometry\n",
    "packages[\"indentfirst\"] = []\n",
    "packages[\"pifont\"] = []\n",
    "packages[\"footmisc\"] = {\"declarations\": [\"perpage\", \"symbol*\"]}\n",
    "lineskip = \"24pt\"\n",
    "parskip = \"6pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小学诗歌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shi_to_tex_str(shi, print_genre=False, authors={}, typesettings={\"vspaces\": {\"after_title\": 8, \"after_author\": 6, \"after_content\": 6}}):\n",
    "    # convert structured shi to string ready to use in tex\n",
    "    out = r\"\\section{\" + shi[\"title\"] + \"}\\n\\n\"\n",
    "    content = \"\"\n",
    "    # title = wrap_method(\"textbf\", zihao(3) + \" \" + shi[\"title\"]) + \"\\n\\n\"\n",
    "\n",
    "    # if print_genre:\n",
    "    #     title = shi[\"genre\"] + \"：\" + title\n",
    "    # content += title\n",
    "    content += wrap_method(\"vspace\", f\"{typesettings['vspaces']['after_title']}pt\") + \"\\n\\n\"\n",
    "    author_str = \"\"\n",
    "    if shi[\"author\"]:\n",
    "        author = shi[\"author\"]\n",
    "        if author in authors:\n",
    "            author_str += \"〔唐代：\" + author + \"〕\\n\\n\"\n",
    "        else:\n",
    "            author_str += \"〔\" + author + \"〕\\n\\n\"\n",
    "    else:\n",
    "        author_str += \"〔作者不详〕\\n\\n\"\n",
    "    content += wrap_env(\"normalsize\", \"\\n\" + author_str) + \"\\n\"\n",
    "    content += wrap_method(\"vspace\", f\"{typesettings['vspaces']['after_author']}pt\") + \"\\n\\n\"\n",
    "    content += wrap_env(\"large\", \"\\n\" + \"\\n\\n\".join([wrap_method(\"xpinyin*\", line) for line in shi[\"content\"]]) + \"\\n\\n\") + \"\\n\"\n",
    "    content = wrap_env(\"center\", content) + \"\\n\"\n",
    "    content += wrap_method(\"vspace\", f\"{typesettings['vspaces']['after_content']}pt\") + \"\\n\\n\"\n",
    "    out += content\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shis = load_cn_json(os.path.join(path_xx, \"古诗.json\"))\n",
    "output_tex = \"古诗集.tex\"\n",
    "title = \"小学语文古诗集\"\n",
    "# shis = load_cn_json(os.path.join(path_xx, \"唐诗三百首.json\"))\n",
    "# output_tex = \"唐诗三百首.tex\"\n",
    "# title = \"唐诗三百首\"\n",
    "\n",
    "header, footer = make_ctex_env(packages=packages, title=title, parskip=parskip, lineskip=lineskip)\n",
    "\n",
    "# 分层\n",
    "shi_by_level = {}\n",
    "levels = []\n",
    "for i in range(10):\n",
    "    levels.append(f\"第{数转中[i+1]}层\") \n",
    "levels.append(\"其他\")\n",
    "# print(levels)\n",
    "\n",
    "typesettings = {\"vspaces\": {\"after_title\": 10, \"after_author\": 8, \"after_content\": 8}}\n",
    "\n",
    "for title, shi in shis.items():\n",
    "    level = shi[\"level\"]\n",
    "    if level not in shi_by_level:\n",
    "        shi_by_level[level] = {}\n",
    "    shi_by_level[level][title] = shi\n",
    "\n",
    "with open(output_tex, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(header + \"\\n\")\n",
    "    for level in levels:\n",
    "        f.write(r\"\\chapter\" + wrap(level) + \"\\n\\n\")\n",
    "        for title, shi in shi_by_level[level].items():\n",
    "            f.write(shi_to_tex_str(shi, typesettings=typesettings) + \"\\n\")\n",
    "    f.write(footer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xiezi_to_str(zis, ncol=2, nex=3, hspace=1):\n",
    "    out = \"\\n\\n\"\n",
    "    boxes = \"\"\n",
    "    i = 0\n",
    "    for zi in zis:\n",
    "        boxes += wrap_method(\"hanzibox\", zi)\n",
    "        for j in range(nex):\n",
    "            boxes += wrap_method(\"hanzibox\", \"\")\n",
    "        i += 1\n",
    "        if i == ncol:\n",
    "            boxes += \"\\n\\n\"\n",
    "            i = 0\n",
    "        else:\n",
    "            boxes += wrap_method(\"hspace\", f\"{hspace}em\")\n",
    "    out += wrap_env(\"center\", boxes + \"\\n\\n\")\n",
    "    return out\n",
    "\n",
    "print(xiezi_to_str(texts_sz[\"一二三四五\"][\"xiezi\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小学现代文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(path, format=\"散文\"):\n",
    "    \"\"\"Read raw text and formalize to json\n",
    "    Inputs: \n",
    "    path (str): file path to the raw text.\n",
    "    format (str): format of the text.\n",
    "    Output:\n",
    "    out (dict): a jsonifiable dictionary with formalized text.\n",
    "    Example:\n",
    "    out[\"format\"]     : format of the text (in the sense of tex printing).\n",
    "    out[\"genre\"]      : genre and other tags of the text.\n",
    "    out[\"content\"]    : content of the text. A list of strings.\n",
    "    out[\"grade\"]      : recommanded student grade (for the purpose of eduation).\n",
    "    out[\"title\"]      : title of the text.\n",
    "    out[\"author\"]     : author of the text.\n",
    "    out[\"remarks\"]    : remarks concerning the text.\n",
    "    out[\"footnotes\"]  : footnotes of the content of the text.\n",
    "    out[\"endnotes\"]   : endnotes of the content of the text.\n",
    "    out[\"vocabulary\"] : vocabulary to learn (for the purpose of eduation).\n",
    "    \"\"\"\n",
    "    lines = read(path)\n",
    "    out = {}\n",
    "    title = \"\"\n",
    "    if len(lines) and len(lines[0]):\n",
    "        author = \"\"\n",
    "        grade = 0\n",
    "        footnotes = []\n",
    "        endnotes = []\n",
    "        vocabulary = []\n",
    "        remarks = []\n",
    "        content = []\n",
    "        out[\"format\"] = format\n",
    "        out[\"genre\"] = [format]\n",
    "        # return lines\n",
    "        if format in (\"散文\", \"书信\", \"小说\", \"剧本\"):\n",
    "            for line in lines:\n",
    "                line0 = line.strip()\n",
    "                if line0:\n",
    "                    if not title:\n",
    "                        title = line0\n",
    "                    elif grade < 1 and line.startswith(\"年级：\"):\n",
    "                        grade = int(line0[3:])\n",
    "                    elif not author and line.startswith(\"作者：\"):\n",
    "                        author = line0[3:]\n",
    "                    elif line.startswith(\"备注：\"):\n",
    "                        remarks.append(line0[3:])\n",
    "                    elif line.startswith(\"注释：\"):\n",
    "                        footnotes.append(line0[3:])\n",
    "                    elif line.startswith(\"脚注：\"):\n",
    "                        footnotes.append(line0[3:])\n",
    "                    elif line.startswith(\"尾注：\"):\n",
    "                        endnotes.append(line0[3:])\n",
    "                    elif line.startswith(\"词汇：\"):\n",
    "                        vocabulary.extend(line0[3:].split())\n",
    "                    else:\n",
    "                        content.append(line0)\n",
    "        elif format == \"诗歌\":\n",
    "            para = []\n",
    "            for line in lines:\n",
    "                line0 = line.strip()\n",
    "                if line0:\n",
    "                    if not title:\n",
    "                        title = line0\n",
    "                    elif grade < 1 and line.startswith(\"年级：\"):\n",
    "                        grade = int(line0[3:])\n",
    "                    elif not author and line.startswith(\"作者：\"):\n",
    "                        author = line0[3:]\n",
    "                    elif line.startswith(\"备注：\"):\n",
    "                        remarks.append(line0[3:])\n",
    "                    elif line.startswith(\"注释：\"):\n",
    "                        footnotes.append(line0[3:])\n",
    "                    elif line.startswith(\"脚注：\"):\n",
    "                        footnotes.append(line0[3:])\n",
    "                    elif line.startswith(\"尾注：\"):\n",
    "                        endnotes.append(line0[3:])\n",
    "                    elif line.startswith(\"词汇：\"):\n",
    "                        vocabulary.extend(line0[3:].split())\n",
    "                    else:\n",
    "                        para.append(line0)\n",
    "                elif len(para):\n",
    "                    content.append(\"|#|\".join(para))\n",
    "                    para = []\n",
    "            if len(para):\n",
    "                content.append(\"|#|\".join(para))\n",
    "        # make footnotes dict\n",
    "        footdict = {}\n",
    "        i = 0\n",
    "        keybase = \"fn\"\n",
    "        content = \"@\".join(content)\n",
    "        for note in footnotes:\n",
    "            word = \"\"\n",
    "            if note.startswith(\"〔\"):\n",
    "                word = note.split(\"〕\")[0][1:]\n",
    "                key = keybase + str(i+1)\n",
    "                footdict[key] = note\n",
    "                i += 1\n",
    "            elif \"〕\" in note:  # key is already marked in the text with the format \"\\apost{a...}\".\n",
    "                key = note.split(\"〕\")[0].split(\"〔\")[0]\n",
    "                footdict[key] = \"\".join(note.split(key)[1:])\n",
    "            # print(word)\n",
    "            if word:  # find the position to insert footnote and mark\n",
    "                nfin = content.find(word) + len(word)\n",
    "                content = content[:nfin] + r\"\\apost{\" + key + \"}\" + content[nfin:]\n",
    "        if \"|#|\" in content:\n",
    "            content_new = []\n",
    "            for para in content.split(\"@\"):\n",
    "                content_new.append(para.split(\"|#|\"))\n",
    "            content = content_new\n",
    "        else:\n",
    "            content = content.split(\"@\")\n",
    "        \n",
    "        out[\"title\"] = title\n",
    "        out[\"author\"] = author\n",
    "        out[\"content\"] = content\n",
    "        out[\"remarks\"] = remarks\n",
    "        out[\"footnotes\"] = footdict\n",
    "        out[\"endnotes\"] = endnotes\n",
    "        out[\"vocabulary\"] = vocabulary\n",
    "        if grade:\n",
    "            out[\"grade\"] = grade\n",
    "    return title, out\n",
    "\n",
    "def text_content_to_tex_str(text, verbose=0, verseprop=0.5, format=\"散文\", footnotes={}, endnotes=[]):\n",
    "    \"\"\"convert the content of a text to text string ready for tex.\n",
    "    the format varies by genre:\n",
    "    散文、小说\n",
    "    书信\n",
    "    诗歌\n",
    "    剧本\n",
    "    \"\"\" \n",
    "    content = text[\"content\"]\n",
    "    if \"footnotes\" in text:\n",
    "        footnotes = text[\"footnotes\"]\n",
    "    if \"format\" in text:\n",
    "        format = text[\"format\"]\n",
    "    out = \"\"\n",
    "    if format in (\"散文\", \"小说\",):\n",
    "        out = \"\\n\\n\".join(content) + \"\\n\"\n",
    "    elif  format == \"书信\":\n",
    "        if verbose and not content[0].endswith(\"：\"):\n",
    "            print(\"错误：第一行不是抬头\")\n",
    "            return \"格式错误\\n\"\n",
    "        out = r\"\\noindent \" + content[0] + \"\\n\\n\" + wrap_method(\"vspace\", \"24pt\") + \"\\n\\n\"\n",
    "        toright = False\n",
    "        toright_content = \"\"\n",
    "        for line in content[1:]:\n",
    "            if line:\n",
    "                if toright:\n",
    "                    toright_content += line + \"\\n\\n\"\n",
    "                else:\n",
    "                    out += line + \"\\n\\n\"\n",
    "            else:\n",
    "                toright = True\n",
    "        out += wrap_method(\"vspace\", \"36pt\") + \"\\n\\n\"\n",
    "        out += wrap_env(\"flushright\", toright_content) + \"\\n\\n\"\n",
    "    elif  format == \"诗歌\":\n",
    "        if not isinstance(content[0], list):\n",
    "            content = [content]\n",
    "        \n",
    "        lineskip = \" \\\\\\\\\\n\"\n",
    "        # parskip = \"\\n\" + wrap_method(\"vspace\", \"4pt\") + \"\\n\\n\"\n",
    "        parskip = \"\\n\\n\"\n",
    "        out = parskip.join([wrap_env(\"verse\", lineskip.join(par) + \"\\n\", params=[str(verseprop)+\"\\\\linewidth\"]) for par in content])\n",
    "    elif format == \"剧本\":\n",
    "        name_set = text[\"characters\"]\n",
    "        for line in content:\n",
    "            if line.startswith(\"\\\\item[\"):\n",
    "                name = line.split(\"]\")[0][6:]\n",
    "                colored_name = r\"{\\color{\" + name_set[name] + r\"} \" + name + r\"}\"\n",
    "                out += \"\\\\item[\" + colored_name + \"]\" + \"]\".join(line.split(\"]\")[1:])\n",
    "            elif line.startswith(\"$\"):\n",
    "                colored_line = line\n",
    "                for name in name_set:\n",
    "                    colored_line = colored_line.replace(name, r\"{\\color{\" + name_set[name] + r\"} \" + name + r\"}\")\n",
    "                out += colored_line\n",
    "            else:\n",
    "                out += line\n",
    "            out += \"\\n\\n\"\n",
    "    for key in footnotes:\n",
    "        out = out.replace(\"apost{\"+key+\"}\", \"footnote{\" + footnotes[key] + \"}\")\n",
    "    return out\n",
    "\n",
    "def endnotes_to_str(endnotes, verbose=0, pinyin=False):\n",
    "    \"\"\"convert the endnotes to text string ready for tex.\"\"\"\n",
    "    out = \"\"\n",
    "    notes = \"\"\n",
    "    for note in endnotes:\n",
    "        if pinyin and note.startswith(\"〔\"):  # add pinyin\n",
    "            suite = note[1:].split(\"〕\")\n",
    "            notes += \"\\item \" + note[0] + r\"\\xpinyin*{\" + suite[0] + r\"}〕\" + \"〕\".join(suite[1:]) + \"\\n\"\n",
    "        else:\n",
    "            notes += \"\\item \" + note + \"\\n\"\n",
    "    if notes:\n",
    "        out = r\"\\newpage\" + \"\\n\\n\" + r\"\\textbf{注释}：\" + \"\\n\\n\" + r\"\\vspace{-1em}\" + \"\\n\\n\"\n",
    "        out += wrap_env(\"itemize\", r\"\\setlength\\itemsep{-0.2em}\" + \"\\n\" + notes)\n",
    "    return out\n",
    "\n",
    "def shizi_to_str(zis, n=8):\n",
    "    out = \"\\clearpage\" + \"\\n\\n\"\n",
    "    boxes = \"\"\n",
    "    i = 0\n",
    "    for zi in zis:\n",
    "        boxes += wrap_method(\"hanzibox\", zi)\n",
    "        i += 1\n",
    "        if i == n:\n",
    "            boxes += \"\\n\\n\"\n",
    "            i = 0\n",
    "    out += wrap_env(\"center\", boxes + \"\\n\\n\")\n",
    "    return out\n",
    "\n",
    "def xiezi_to_str(zis, ncol=2, nex=4, hspace=1):\n",
    "    out = \"\"\n",
    "    boxes = \"\"\n",
    "    i = 0\n",
    "    for zi in zis:\n",
    "        boxes += wrap_method(\"hanzibox\", zi)\n",
    "        for j in range(nex):\n",
    "            boxes += wrap_method(\"hanzibox\", \"\")\n",
    "        i += 1\n",
    "        if i == ncol:\n",
    "            boxes += \"\\n\\n\"\n",
    "            i = 0\n",
    "        else:\n",
    "            boxes += wrap_method(\"hspace\", f\"{hspace}em\")\n",
    "    out += boxes + \"\\n\\n\"\n",
    "    # out += wrap_env(\"center\", boxes + \"\\n\\n\")\n",
    "    return out\n",
    "\n",
    "def text_to_tex_str(text, typesettings={\"font\": {\"title\": {\"size\": 2}, \"plaintext\": {\"size\": \"normalsize\"}}, \"vspaces\": {\"after_title\": 12, \"after_author\": 6, \"after_content\": 6}}):\n",
    "    \"\"\"convert a text object to text string ready for tex\n",
    "    \"\"\"\n",
    "    out = \"\"\n",
    "    content = \"\"\n",
    "    # title_fontsize = typesettings[\"font\"][\"title\"][\"size\"]\n",
    "    # title = wrap_method(\"textbf\", zihao(title_fontsize) + \" \" + text[\"title\"]) + \"\\n\"\n",
    "    title = wrap_method(\"chapter\", text[\"title\"]) + \"\\n\\n\"\n",
    "    content += title\n",
    "    # content = wrap_env(\"center\", content) + \"\\n\"\n",
    "    # content += wrap_method(\"vspace\", f\"{typesettings['vspaces']['after_title']}pt\") + \"\\n\\n\"\n",
    "    content += wrap_env(typesettings[\"font\"][\"plaintext\"][\"size\"], \"\\n\" + text_content_to_tex_str(text) + \"\\n\")\n",
    "    # content += wrap_method(\"vspace\", f\"{typesettings['vspaces']['after_content']}pt\") + \"\\n\\n\"\n",
    "    out += content + \"\\n\\n\"\n",
    "    # out += wrap_method(\"newpage\", keep_wrapper=False) + \"\\n\\n\"\n",
    "    if \"endnotes\" in text:\n",
    "        out += endnotes_to_str(text[\"endnotes\"])\n",
    "    if \"shizi\" in text:\n",
    "        nchars = 10\n",
    "        if len(text[\"shizi\"]) % 10 == 1:\n",
    "            nchars = 8\n",
    "        out += shizi_to_str(text[\"shizi\"], n=nchars) + \"\\n\\n\"\n",
    "        if \"xiezi\" in text:\n",
    "            out += xiezi_to_str(text[\"xiezi\"]) + \"\\n\\n\"\n",
    "    return out\n",
    "\n",
    "def add_text(texts, title, content, format=\"散文\", tags=[]):\n",
    "    \"\"\"Add a text to the dictionary of texts.\n",
    "    Inputs:\n",
    "    texts (dict): dictionary of texts. title --> content.\n",
    "    title (str): title of the text.\n",
    "    content (dict): content of the text.\n",
    "    format (str): format of the text.\n",
    "    tags (list of str): tags to describe the text.\n",
    "    Output:\n",
    "    texts: updated dictionary of texts. \n",
    "    \"\"\"\n",
    "    if len(tags):\n",
    "        content[\"genre\"] = tags\n",
    "    if format == \"剧本\":\n",
    "        if title not in texts:\n",
    "            script_keys = []\n",
    "            for _, text in texts.items():\n",
    "                if text[\"format\"] == \"剧本\" and \"key\" in text:\n",
    "                    script_keys.append(int(text[\"key\"].split(\"-\")[1]))\n",
    "            if len(script_keys):\n",
    "                script_key = \"script-\" + str(max(script_keys) + 1)\n",
    "            else:\n",
    "                script_key = \"script-1\"\n",
    "                \n",
    "        else:\n",
    "            script_key = texts[title][\"key\"]\n",
    "        name_set, color_set = set_char_colors(content[\"content\"], script_key)\n",
    "        \n",
    "    texts[title] = content\n",
    "    if format == \"剧本\":\n",
    "        texts[title][\"key\"] = script_key\n",
    "        texts[title][\"characters\"] = name_set\n",
    "        texts[title][\"character_colors\"] = color_set\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = {}\n",
    "packages[\"ctex\"] = []\n",
    "packages[\"titlesec\"] = []\n",
    "packages[\"xeCJK\"] = []\n",
    "packages[\"verse\"] = []\n",
    "packages[\"fontspec,xunicode,xltxtra\"] = []\n",
    "packages[\"xpinyin\"] = pinyin\n",
    "packages[\"geometry\"] = geometry\n",
    "packages[\"indentfirst\"] = []\n",
    "packages[\"pifont\"] = []\n",
    "packages[\"footmisc\"] = {\"declarations\": [\"perpage\", \"symbol*\"]}\n",
    "lineskip = \"24pt\"\n",
    "parskip = \"6pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_xx = load_cn_json(\"../src/小学/阅读课文.json\")\n",
    "\n",
    "text_format = \"剧本\"\n",
    "tags = [\"戏曲\", \"节选\", \"名著\", \"古白话文\", \"悲剧\"]\n",
    "title, content = read_text(\"草稿.tex\", format=text_format)\n",
    "\n",
    "if title:\n",
    "    print(f\"新增课文：{title}\")\n",
    "    texts = add_text(texts_xx, title, content, text_format, tags)\n",
    "    dump_cn_json(\"../src/小学/阅读课文.json\", texts_xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_xx = load_cn_json(\"../src/小学/阅读课文.json\")\n",
    "\n",
    "booktitle = \"小学语文课文集萃\"\n",
    "header, footer = make_ctex_env(packages=packages, title=booktitle, parskip=parskip, lineskip=lineskip)\n",
    "typesettings = {}\n",
    "typesettings[\"vspaces\"] = {\"after_title\": 36, \"after_author\": 16, \"after_content\": 16}\n",
    "typesettings[\"font\"] = {\"plaintext\": {\"size\": \"large\"}}\n",
    "\n",
    "with open(\"小学现代文阅读课文.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(header + \"\\n\")\n",
    "    for title, text in sort_dict_with(texts_xx):\n",
    "        # print(title)\n",
    "        if text[\"grade\"] > 0:\n",
    "            f.write(text_to_tex_str(text, typesettings=typesettings) + \"\\n\")\n",
    "    f.write(footer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 年级 (16)\n",
      "['狼和小羊', '翠鸟', '揠苗助长', '守株待兔', '初冬']\n",
      "['秋天', '坐井观天', '骆驼和羊', '狐狸和乌鸦', '曹冲称象']\n",
      "['乌鸦喝水', '狐狸和公鸡', '老狼分饼', '叶公好龙', '十二月花名歌']\n",
      "['画蛇添足']\n",
      "3 年级 (48)\n",
      "['茅以升立志造桥', '美丽的小兴安岭', '大海的歌', '让我们荡起双桨', '小马过河']\n",
      "['刻舟求剑 ', '八角楼上', '赵州桥', '南京长江大桥', '雨']\n",
      "['放风筝', '荷花', '掩耳盗铃', '自相矛盾', '滥竽充数']\n",
      "['惊弓之鸟', '绿色的办公室', '黄继光', '颐和园', '五彩池']\n",
      "['青蛙的眼睛', '爬山虎的脚', '课间十分钟', '日出', '捞铁牛']\n",
      "['纸上谈兵', '趵突泉', '鸟的天堂', '桂林山水', '天安门广场']\n",
      "['火烧云', '卢沟桥的狮子', '海上日出', '董存瑞舍身炸碉堡', '十里长街送总理']\n",
      "['狐狸和山羊', '燕子', '晏子使楚', '狼牙山五壮士', '我的战友邱少云']\n",
      "['草原', '马踏飞燕', '伏尔加河上的纤夫', '牛郎织女的故事', '搭船的鸟']\n",
      "['狐假虎威', '塞翁失马', '买椟还珠']\n",
      "4 年级 (54)\n",
      "['我和企鹅', '白求恩大夫（节选改编）', '我的弟弟“小萝卜头”', '帐篷', '参观人民大会堂']\n",
      "['海底世界', '故乡的杨梅', '杏儿熟了', '春蚕', '李时珍']\n",
      "['画杨桃', '珍贵的教科书', '爸爸和书', '小珊迪', '劳动最有滋味']\n",
      "['花生花', '种子', '观潮', '高大的皂荚树', '海滨小城']\n",
      "['蝙蝠和雷达', '各种各样的玻璃', '糖画', '西门豹', '中国石']\n",
      "['古井', '峨眉道上', '太阳', '绿叶', '九寨沟']\n",
      "['兵马俑', '冬眠', '七月的天山', '小英雄雨来', '参观刘家峡水电站']\n",
      "['小站', '挑山工', '可爱的草塘', '雪猴', '鲸']\n",
      "['圆明园的毁灭', '喂药（汤姆索亚历险记节选）', '阁楼（小公主节选）', '冀中的地道战', '草船借箭']\n",
      "['田忌赛马', '记金华的双龙洞', '丰碑', '镜泊湖奇观', '伟大的友谊']\n",
      "['詹天佑', '东郭先生与狼', '瑞雪图', '彭德怀速写']\n",
      "5 年级 (41)\n",
      "['长城', '森林的主人', '鱼游到了纸上', '蟋蟀的住宅', '高梁情']\n",
      "['只有一个地球', '一个苹果', '出海（老人与海节选）', '养花', '蛇与庄稼']\n",
      "['彩色的翅膀', '飞夺泸定桥', '囚歌', '我的“自白”书', '给颜黎民的信']\n",
      "['落花生', '难忘的一课', '毛主席在花山', '开国大典', '狱中联欢']\n",
      "['凡卡', '金色的鱼钩', '一夜的工作', '林海', '第一场雪']\n",
      "['灯光', '卖火柴的小女孩', '将相和', '景阳冈', '梅花魂']\n",
      "['为人民服务', '难忘的启蒙', '珊瑚', '三亚落日', '富饶的西沙群岛']\n",
      "['带刺的朋友', '科学家竺可桢', '白鹭', '向祖国致敬', '王二小']\n",
      "['啊，姑娘再见！']\n",
      "6 年级 (23)\n",
      "['菩萨蛮·大柏地', '穷人', '琥珀', '十六年前的回忆', '夜莺的歌声']\n",
      "['花潮', '少年闰土', '青山', '草方格', '北京的春节']\n",
      "['他们那时候多有趣啊', '故宫博物院', '海的颜色', '有的人', '冬天的济南']\n",
      "['撤离班加西', '北约轰炸大使馆', '在巴黎传递奥运火炬', '美猴王当弼马温', '海滨仲夏夜']\n",
      "['延安的秋天', '绿宝团', '我为少男少女们歌唱']\n"
     ]
    }
   ],
   "source": [
    "texts_xx = load_cn_json(\"../src/小学/阅读课文.json\")\n",
    "\n",
    "grade_count = {}\n",
    "title_by_grade = {}\n",
    "title_by_genre = {}\n",
    "genre_by_grade = {}\n",
    "for title, text in texts_xx.items():\n",
    "    if \"grade\" not in text:\n",
    "        text[\"grade\"] = 1\n",
    "    g = text[\"grade\"]\n",
    "    if g not in grade_count:\n",
    "        grade_count[g] = 0\n",
    "    if g not in title_by_grade:\n",
    "        title_by_grade[g] = []\n",
    "    if g not in genre_by_grade:\n",
    "        genre_by_grade[g] = {}\n",
    "    for genre in text[\"genre\"]:\n",
    "        if genre not in genre_by_grade[g]:\n",
    "            genre_by_grade[g][genre] = []\n",
    "        if genre not in title_by_genre:\n",
    "            title_by_genre[genre] = []\n",
    "    grade_count[text[\"grade\"]] += 1\n",
    "    title_by_grade[text[\"grade\"]].append(title)\n",
    "    for genre in text[\"genre\"]:\n",
    "        genre_by_grade[g][genre].append(title)\n",
    "        title_by_genre[genre].append(title)\n",
    "\n",
    "for g in title_by_grade:\n",
    "    print(f\"{g} 年级 ({len(title_by_grade[g])})\")\n",
    "    nice_print(title_by_grade[g])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中学"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_xx = load_cn_json(\"../src/小学/阅读课文.json\")\n",
    "texts_cz = load_cn_json(\"../src/中学/阅读课文.json\")\n",
    "\n",
    "titles1 = set(list(texts_xx.keys()))\n",
    "titles2 = set(list(texts_cz.keys()))\n",
    "\n",
    "titles1 & titles2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 现代文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = {}\n",
    "packages[\"ctex\"] = []\n",
    "packages[\"titlesec\"] = []\n",
    "packages[\"xeCJK\"] = []\n",
    "packages[\"verse\"] = []\n",
    "packages[\"fontspec,xunicode,xltxtra\"] = []\n",
    "packages[\"xpinyin\"] = pinyin\n",
    "packages[\"geometry\"] = geometry\n",
    "packages[\"indentfirst\"] = []\n",
    "packages[\"pifont\"] = []\n",
    "packages[\"enumitem\"] = []\n",
    "packages[\"footmisc\"] = {\"declarations\": [\"perpage\", \"symbol*\"]}\n",
    "xcolor = {}\n",
    "xcolor[\"declarations\"] = [\"table\", \"dvipsnames\"]\n",
    "packages[\"xcolor\"] = xcolor\n",
    "\n",
    "typesettings = {}\n",
    "typesettings[\"vspaces\"] = {\"after_title\": 36, \"after_author\": 16, \"after_content\": 16}\n",
    "typesettings[\"font\"] = {\"plaintext\": {\"size\": \"normalsize\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_zs(path):\n",
    "    lines = read(\"草稿.tex\")\n",
    "\n",
    "    out = []\n",
    "    zs = False\n",
    "    for line in lines:\n",
    "        if line[0] in \"0987654321\":\n",
    "            zs = True\n",
    "            continue\n",
    "        if zs:\n",
    "            parts = line.split(\"：\")\n",
    "            out.append(f\"注释：〔{parts[0]}〕\" + \"：\".join(parts[1:]))\n",
    "            zs = False\n",
    "        else:\n",
    "            newline =\"\".join([w for w in line if w not in \"0987654321\"])\n",
    "            out.append(newline)\n",
    "\n",
    "    with open(\"草稿.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_cz = load_cn_json(\"../src/中学/阅读课文.json\")\n",
    "\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "text_format = \"剧本\"\n",
    "tags = [\"戏曲\", \"节选\", \"名著\", \"古白话文\", \"悲剧\"]\n",
    "title, content = read_text(\"草稿.tex\", format=text_format)\n",
    "\n",
    "if title:\n",
    "    print(f\"新增课文：{title}\")\n",
    "    texts = add_text(texts_cz, title, content, text_format, tags)\n",
    "    dump_cn_json(\"../src/中学/阅读课文.json\", texts_cz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "booktitle = \"中学语文课文集萃\"\n",
    "texts_cz = load_cn_json(\"../src/中学/阅读课文.json\")\n",
    "\n",
    "lineskip = \"24pt\"\n",
    "parskip = \"6pt\"\n",
    "package_update_xcolor(packages, texts_cz)\n",
    "header, footer = make_ctex_env(packages=packages, title=booktitle, parskip=parskip, lineskip=lineskip)\n",
    "with open(\"中学现代文阅读课文.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(header + \"\\n\")\n",
    "    for title, text in sort_dict_with(texts_cz):\n",
    "        # print(title)\n",
    "        if text[\"grade\"] > 0:\n",
    "            f.write(text_to_tex_str(text, typesettings=typesettings) + \"\\n\")\n",
    "        # break\n",
    "    f.write(footer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-20716fbf70da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mgrade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrade_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgrade_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnice_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle_by_grade\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgrade\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-20716fbf70da>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mgrade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrade_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgrade_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnice_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle_by_grade\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgrade\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 7"
     ]
    }
   ],
   "source": [
    "grade = 11\n",
    "print([(g, grade_count[g]) for g in range(7, 13)])\n",
    "print(sum([grade_count[g] for g in range(7, 13)]))\n",
    "nice_print(title_by_grade[grade])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从百草园到三味书屋\n",
      "阿长与山海经\n",
      "论雷峰塔的倒掉\n",
      "“友邦惊诧”论\n",
      "社戏\n",
      "故乡\n",
      "藤野先生\n",
      "孔乙己\n",
      "中国人失掉自信力了吗\n",
      "拿来主义\n",
      "祝福\n",
      "聪明人和傻子和奴才\n",
      "记念刘和珍君\n",
      "《呐喊》自序\n",
      "药\n",
      "阿Q正传（节选）\n"
     ]
    }
   ],
   "source": [
    "for title, text in texts_cz.items():\n",
    "    if text[\"author\"] == \"鲁迅\":\n",
    "        if \"节选\" in text[\"genre\"]:\n",
    "            print(title+ \"（节选）\")\n",
    "        else:\n",
    "            print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 ['记叙文']\n",
      "55 ['散文']\n",
      "26 ['说明文']\n",
      "25 ['寓言']\n",
      "23 ['描写文']\n",
      "20 ['小说']\n",
      "18 ['回忆', '文言文翻译']\n",
      "16 ['人物', '地方介绍']\n",
      "14 ['成语故事']\n",
      "12 ['报告文学']\n",
      "11 ['科普']\n",
      "10 ['游记']\n",
      "9 ['名人故事']\n",
      "8 ['抒情']\n",
      "7 ['诗歌']\n",
      "5 ['动物']\n",
      "4 ['事物介绍', '纪实文学', '描写']\n",
      "3 ['人物介绍', '借事说理', '议论文', '童话', '纪实']\n",
      "2 ['借物喻理', '名著', '节选', '写景', '歌词', '民谣']\n",
      "1 ['古文翻译', '笔记', '借物抒情', '书信', '科幻', '神话传说', '幻想', '经典', '声明', '应用文', '言志', '民俗', '植物', '时令', '儿歌', '白话文']\n"
     ]
    }
   ],
   "source": [
    "texts_xx = load_cn_json(\"../src/小学/阅读课文.json\")\n",
    "grade_count = {}\n",
    "title_by_grade = {}\n",
    "title_by_genre = {}\n",
    "genre_by_grade = {}\n",
    "for title, text in texts_xx.items():\n",
    "    if \"grade\" not in text:\n",
    "        text[\"grade\"] = 1\n",
    "    g = text[\"grade\"]\n",
    "    if g not in grade_count:\n",
    "        grade_count[g] = 0\n",
    "    if g not in title_by_grade:\n",
    "        title_by_grade[g] = []\n",
    "    if g not in genre_by_grade:\n",
    "        genre_by_grade[g] = {}\n",
    "    for genre in text[\"genre\"]:\n",
    "        if genre not in genre_by_grade[g]:\n",
    "            genre_by_grade[g][genre] = []\n",
    "        if genre not in title_by_genre:\n",
    "            title_by_genre[genre] = []\n",
    "    grade_count[text[\"grade\"]] += 1\n",
    "    title_by_grade[text[\"grade\"]].append(title)\n",
    "    for genre in text[\"genre\"]:\n",
    "        genre_by_grade[g][genre].append(title)\n",
    "        title_by_genre[genre].append(title)\n",
    "\n",
    "res = [(k,len(v)) for k, v in title_by_genre.items()]\n",
    "\n",
    "# alist = res\n",
    "def printsort_int(alist, rev=False):\n",
    "    ma = max([b for (_, b) in alist])\n",
    "    tem = [[] for _ in range(ma+1)]\n",
    "    for (a, b) in alist:\n",
    "        tem[b].append(a)\n",
    "    \n",
    "    if rev:\n",
    "        for i, a in enumerate(tem[::-1]):\n",
    "            if len(a):\n",
    "                print(ma-i, a)\n",
    "    else:\n",
    "        for i, a in enumerate(tem):\n",
    "            if len(a):\n",
    "                print(i, a)\n",
    "\n",
    "printsort_int(res, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 ['小说']\n",
      "28 ['散文']\n",
      "26 ['经典']\n",
      "23 ['抒情', '诗歌']\n",
      "19 ['节选']\n",
      "17 ['议论文']\n",
      "16 ['自然', '现实主义', '浪漫主义']\n",
      "15 ['回忆']\n",
      "14 ['说明文']\n",
      "12 ['景物', '科普']\n",
      "11 ['人物']\n",
      "10 ['批判现实主义']\n",
      "9 ['演讲', '意象']\n",
      "8 ['倡议', '叙事']\n",
      "7 ['记叙文', '论述', '象征主义']\n",
      "6 ['社会百态']\n",
      "5 ['纪实', '讽刺']\n",
      "4 ['四季', '建筑', '鼓动']\n",
      "3 ['名著', '游记']\n",
      "2 ['童话', '借物抒情', '动物', '报告文学', '书信', '驳论', '议论', '想象', '序言', '话剧', '意识流', '说理']\n",
      "1 ['悼词', '抗议', '友情', '纪实文学', '植物', '亲情', '提出问题', '戏剧', '墙头诗', '新闻稿', '对话录', '科学', '推理', '辩论', '批评', '传记', '寓言', '纪念', '都市童话', '插叙', '回信', '阐述', '剧本', '地方介绍', '公开信']\n"
     ]
    }
   ],
   "source": [
    "# [title for title in texts_cz if texts_cz[title]['author'] == \"鲁迅\"]\n",
    "texts_cz = load_cn_json(\"../src/中学/阅读课文.json\")\n",
    "grade_count = {}\n",
    "title_by_grade = {}\n",
    "title_by_genre = {}\n",
    "genre_by_grade = {}\n",
    "for title, text in texts_cz.items():\n",
    "    g = text[\"grade\"]\n",
    "    if g not in grade_count:\n",
    "        grade_count[g] = 0\n",
    "    if g not in title_by_grade:\n",
    "        title_by_grade[g] = []\n",
    "    if g not in genre_by_grade:\n",
    "        genre_by_grade[g] = {}\n",
    "    for genre in text[\"genre\"]:\n",
    "        if genre not in genre_by_grade[g]:\n",
    "            genre_by_grade[g][genre] = []\n",
    "        if genre not in title_by_genre:\n",
    "            title_by_genre[genre] = []\n",
    "    grade_count[text[\"grade\"]] += 1\n",
    "    title_by_grade[text[\"grade\"]].append(title)\n",
    "    for genre in text[\"genre\"]:\n",
    "        genre_by_grade[g][genre].append(title)\n",
    "        title_by_genre[genre].append(title)\n",
    "\n",
    "res = [(k,len(v)) for k, v in title_by_genre.items()]\n",
    "\n",
    "# alist = res\n",
    "def printsort_int(alist, rev=False):\n",
    "    ma = max([b for (_, b) in alist])\n",
    "    tem = [[] for _ in range(ma+1)]\n",
    "    for (a, b) in alist:\n",
    "        tem[b].append(a)\n",
    "    \n",
    "    if rev:\n",
    "        for i, a in enumerate(tem[::-1]):\n",
    "            if len(a):\n",
    "                print(ma-i, a)\n",
    "    else:\n",
    "        for i, a in enumerate(tem):\n",
    "            if len(a):\n",
    "                print(i, a)\n",
    "\n",
    "printsort_int(res, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{7: 43, 8: 36, 9: 24, 10: 15, 12: 2, 11: 1}"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nice_print(title_by_genre[\"自然\"])\n",
    "grade_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(text):\n",
    "    out = ''\n",
    "    content = text['content']\n",
    "    if text['format'] == '诗歌':\n",
    "        for block in content:\n",
    "            for line in block:\n",
    "                out += line\n",
    "    else:\n",
    "        for line in content:\n",
    "            out += line\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_cz = load_cn_json(\"../src/小学/阅读课文.json\")\n",
    "\n",
    "for title, text in texts_cz.items():\n",
    "    if text[\"format\"] == \"诗歌\":\n",
    "        continue\n",
    "    footnotes = text['footnotes']\n",
    "    foots_new = {}\n",
    "    keybase = \"fn\"\n",
    "    content = text['content']\n",
    "    content_new = []\n",
    "    s = get_content(text)\n",
    "    if s.find(\"footnote\") < 0:  # no foonote numerated\n",
    "        foots_new = {}\n",
    "        i = 0\n",
    "        out = \"|#|\".join(text[\"content\"])\n",
    "        for note in footnotes:\n",
    "            word = \"\"\n",
    "            if note.startswith(\"〔\"):\n",
    "                word = note.split(\"〕\")[0][1:]\n",
    "                key = keybase + str(i+1)\n",
    "                foots_new[key] = note\n",
    "                i += 1\n",
    "            elif \"〕\" in note:  # key is already marked in the text with the format \"\\apost{a...}\".\n",
    "                key = note.split(\"〕\")[0].split(\"〔\")[0]\n",
    "                foots_new[key] = \"\".join(note.split(key)[1:])\n",
    "            # print(word)\n",
    "            if word:  # find the position to insert footnote and mark\n",
    "                nfin = out.find(word) + len(word)\n",
    "                out = out[:nfin] + r\"\\apost{\" + key + \"}\" + out[nfin:]\n",
    "        content_new = out.split(\"|#|\")\n",
    "    else:  # footnote numerated\n",
    "        for i, note in enumerate(footnotes):\n",
    "            key = keybase + str(i+1)\n",
    "            foots_new[key] = note\n",
    "        i = 0  # counter for line\n",
    "        j = 0  # counter for note\n",
    "        line = content[i]\n",
    "        while i < len(content) and j < len(footnotes):\n",
    "            if line.find(\"footnote{\"+str(j+1)+\"}\") < 0:  # if you cannot find a note in this line\n",
    "                content_new.append(line)  # get to original line\n",
    "                i += 1\n",
    "                line = content[i]  # load the next line\n",
    "            else:  # if you find a note in this line \n",
    "                key = keybase + str(j+1)\n",
    "                line = line.replace(\"footnote{\"+str(j+1)+\"}\", \"apost{\" + key + \"}\")  # replace\n",
    "                j += 1  # move to the next note\n",
    "        content_new.append(line)\n",
    "        i += 1\n",
    "        while i < len(content):\n",
    "            line = content[i]\n",
    "            content_new.append(line)\n",
    "            i += 1            \n",
    "            \n",
    "    text['footnotes'] = foots_new\n",
    "    text['content'] = content_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_cn_json(\"../src/小学/阅读课文.json\", texts_cz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = load_cn_json(\"../src/小学/阅读课文.json\")\n",
    "\n",
    "for title, text in texts.items():\n",
    "    if isinstance(text[\"footnotes\"], list):\n",
    "        text[\"footnotes\"] = {}\n",
    "\n",
    "dump_cn_json(\"../src/小学/阅读课文.json\", texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['监斩官',\n",
       " '刽子',\n",
       " '正旦',\n",
       " '刽子',\n",
       " '正旦',\n",
       " '刽子',\n",
       " '正旦',\n",
       " '卜儿',\n",
       " '刽子',\n",
       " '正旦',\n",
       " '刽子',\n",
       " '卜儿',\n",
       " '正旦',\n",
       " '刽子',\n",
       " '正旦',\n",
       " '监斩官',\n",
       " '正旦',\n",
       " '监斩官',\n",
       " '正旦',\n",
       " '刽子',\n",
       " '正旦',\n",
       " '监斩官',\n",
       " '正旦',\n",
       " '正旦',\n",
       " '监斩官',\n",
       " '正旦',\n",
       " '刽子',\n",
       " '刽子',\n",
       " '正旦',\n",
       " '监斩官',\n",
       " '刽子',\n",
       " '监斩官']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = read(\"草稿.tex\")\n",
    "\n",
    "characters = []\n",
    "\n",
    "for line in lines:\n",
    "    line = line.rstrip()\n",
    "    if line.startswith(\"［\") and line.endswith(\"］\") and line[-2] in (\"云\", \"唱\"):\n",
    "        # print(line)\n",
    "        content = line[1:-2]\n",
    "        if content.endswith(\"，\"):\n",
    "            if \"扮\" in content:\n",
    "                character = content.split(\"扮\")[1].split(\"上\")[0]\n",
    "            else:\n",
    "                character = content[:2]\n",
    "            # print(character)\n",
    "            characters.append(character)\n",
    "        else:\n",
    "            if \"，\" in content:\n",
    "                character = content.split(\"，\")[-1]\n",
    "                # print(content)\n",
    "                # print(character)\n",
    "            else:\n",
    "                character = content[:3]\n",
    "                # print(character, content)\n",
    "            characters.append(character)\n",
    "        \n",
    "characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 识字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sz = \"../src/小学/发蒙识字.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人一 人\n",
      "家 我有一个家\n",
      "你开心吗 你开心吗？\n"
     ]
    }
   ],
   "source": [
    "# texts_sz = load_cn_json(\"../src/小学/发蒙识字 copy.json\")\n",
    "# texts_li = load_cn_json(\"../src/小学/发蒙识字.json\")\n",
    "# texts_nu = {}\n",
    "\n",
    "# # for text in texts_li:\n",
    "# #     title = text[\"title\"]\n",
    "# #     for text2 in texts_sz:\n",
    "# #         if text2[\"title\"]\n",
    "# titles = {}\n",
    "# titles_2 = {}\n",
    "# for name, text in texts_sz.items():\n",
    "#     title = text[\"title\"]\n",
    "#     titles[name] = title\n",
    "#     if title in titles_2:\n",
    "#         print(title, name, titles_2[title])\n",
    "#     titles_2[title] = name\n",
    "\n",
    "# for name, title in titles.items():\n",
    "#     if name != title:\n",
    "#         print(name, title)\n",
    "\n",
    "# for text in texts_li:\n",
    "#     name = titles_2[text[\"title\"]]\n",
    "#     texts_nu[name] = text\n",
    "\n",
    "# old = set(list(texts_sz.keys()))\n",
    "# new = set(list(texts_nu.keys()))\n",
    "\n",
    "# # old - new, new - old\n",
    "\n",
    "# for name in titles:\n",
    "#     if texts_sz[name][\"title\"] != texts_nu[name][\"title\"]:\n",
    "#         print(name, texts_sz[name][\"title\"], texts_nu[name][\"title\"])\n",
    "\n",
    "# dump_cn_json_compact(\"../src/小学/发蒙识字.json\", texts_nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_sz = load_cn_json(\"../src/小学/发蒙识字.json\")\n",
    "dump_cn_json_compact(\"../src/小学/发蒙识字.json\", texts_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 我是中国人\n",
      "20 一二三四五\n",
      "30 人\n",
      "40 田\n",
      "50 比大小\n",
      "60 开门\n",
      "70 山村\n",
      "80 你我他\n",
      "90 田鸟\n",
      "100 鸟鱼虫\n",
      "110 山羊\n",
      "120 方向\n",
      "130 我有一个家\n",
      "140 看地图\n",
      "150 工农兵\n",
      "160 张开口\n",
      "170 这是什么\n",
      "180 你吃什么\n",
      "190 谁比我高\n",
      "200 时间\n",
      "210 问路\n",
      "220 过马路\n",
      "230 你叫什么名字\n",
      "240 画\n",
      "250 下雨啦\n",
      "260 小猫读书\n",
      "270 今天天气好\n",
      "280 车子吃什么油\n",
      "290 小舟\n",
      "300 我有一张床\n",
      "310 我的笔盒\n",
      "320 找朋友\n",
      "330 你开心吗？\n",
      "340 切西瓜\n",
      "350 丢手绢\n",
      "360 秋天\n",
      "370 画彩虹\n",
      "380 风筝\n",
      "390 大桥\n",
      "400 中秋\n",
      "410 拍皮球\n",
      "420 你知不知道\n",
      "430 雪地里的小画家\n",
      "440 二月二\n",
      "450 菜市场\n",
      "460 大扫除\n",
      "470 东海龙宫\n",
      "480 江南\n",
      "490 鹅\n",
      "500 春天在哪里\n",
      "510 看星星\n",
      "520 小燕子\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "texts_sz = load_cn_json(\"../src/小学/发蒙识字 copy.json\")\n",
    "\n",
    "# arrange the texts by numero\n",
    "nx = np.sort(np.array([text['numero'] for idx, text in texts_sz.items()]))\n",
    "texts_nu = []\n",
    "for i in nx:\n",
    "    for idx, text in texts_sz.items():\n",
    "        if text['numero'] == i:\n",
    "            texts_nu.append(text)\n",
    "\n",
    "n = 0\n",
    "b = 10\n",
    "for text in texts_nu:\n",
    "    n += b\n",
    "    text[\"numero\"] = n\n",
    "    print(n, text[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = \"人,一,二,三,四,五,六,七,八,九,十,口,手,足,头,耳,目,牙,日,月,山,水,火,土,天,地,田,雨,中,上,下,大,小,多,少,爸,妈,有,在,个,门,开,学,不,来,去,见,工,农,兵,力\".split(\",\")\n",
    "L2 = \"生,你,他,比,走,跑,跳,叫,吃,喝,看,听,说,青,草,羊,牛,马,鸟,鱼,虫,黄,里,飞,点,方,早,阳,左,右,东,南,西,北,前,后,家,儿,子,男,女,哥,弟,姐,妹,和,只,要,车,电,用,行,道,线,红,绿,灯,牙,几,间,嘴,外,江,船,以,自,能,问,先,往,路,公,园,直,再,就,请,花,黑,白,来,去,见,在,个\".split(\",\")\n",
    "L3 = \"时,间,季,年,秒,现,半,没,高,屋,到,谁,说,凉,群,树,叶,从,字,排,落,啊,要,种,果,发,长,知,广,深,海,老,虎,肉,答,哪,眼,睛,影,湖,唱,雪,鸭,狗,笔,步,用,蛙,洞,为,找,朋,友,笑,床,被,枕,柜,台,衣,服,干,读,书,页,合,气,两,朵,背,包,太,阳,空,今,明,春,秋,哭,买,卖,猫,冬,晴,玩,星,晚,午,菜,饭,米,面,奶,茶,校,店,市,村,河\".split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 简要检查报告 ===\n",
      "教材总课数：53课\n",
      "总识字量：486字\n",
      "总写字量：373字\n",
      "\n",
      "=== 问题汇总 ===\n",
      "1. 重复识字任务：0字\n",
      "2. 重复写字任务：0字\n",
      "3. 写字先于识字：0处\n",
      "4. 内容不匹配：2处\n",
      "5. 未学先现字符：38次\n",
      "\n",
      "=== 识写间隔统计 ===\n",
      "最小距离：0课\n",
      "最大距离：25课\n",
      "平均距离：9.50课\n",
      "中位距离：9.00课\n",
      "- 距离 17课：1字。\n",
      "- 距离 18课：2字。\n",
      "- 距离 19课：3字。\n",
      "- 距离 20课：1字。\n",
      "- 距离 25课：1字。\n",
      "\n",
      "=== 预现统计 ===\n",
      "平均预现距离：7.60课\n",
      "未学字数：13字\n",
      "\n",
      "=== 复现统计 ===\n",
      "平均复现次数：3.6次\n",
      "未复现字数：0字\n",
      "\n",
      "完整报告已保存至：../out/report_发蒙识字_2025-02-07_15-00-34.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def add_suffix(filename, path):\n",
    "    # 获取当前日期并格式化为 YYYYMMDD 格式\n",
    "    time_suffix = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    # 分离文件名和扩展名\n",
    "    name, extension = filename.rsplit('.', 1)\n",
    "    project_name = os.path.splitext(os.path.basename(path))[0]\n",
    "    \n",
    "    # 添加日期后缀并返回新文件名\n",
    "    new_filename = f\"{name}_{project_name}_{time_suffix}.{extension}\"\n",
    "    return new_filename\n",
    "\n",
    "def load_textbook(path):\n",
    "    textbook = load_cn_json(path)\n",
    "    \n",
    "    nx = np.sort(np.array([text['numero'] for _, text in textbook.items()]))\n",
    "    lessons = []\n",
    "    for i in nx:\n",
    "        for _, text in textbook.items():\n",
    "            if text['numero'] == i:\n",
    "                lessons.append(text)\n",
    "    for lesson in lessons:\n",
    "        lesson[\"numero\"] = int((lesson[\"numero\"] + 1) / 10)\n",
    "    \n",
    "    return lessons\n",
    "\n",
    "def analyze_textbook(path, report_path):\n",
    "    \"\"\"教材分析主函数\"\"\"\n",
    "    # 读取并预处理数据\n",
    "    lessons = load_textbook(path)\n",
    "    \n",
    "    # 初始化数据结构\n",
    "    analysis = {\n",
    "        'chars': {\n",
    "            'shizi': defaultdict(list),  # {字: [出现的课号]}\n",
    "            'xiezi': defaultdict(list),\n",
    "            'first_shizi': {},  # 字首次出现在shizi的课号\n",
    "            'first_xiezi': {},\n",
    "            'text_chars': defaultdict(set)  # 每课实际出现的汉字\n",
    "        },\n",
    "        'issues': {\n",
    "            'duplicate_shizi': defaultdict(list),\n",
    "            'duplicate_xiezi': defaultdict(list),\n",
    "            'writing_before_reading': [],\n",
    "            'writing_after_reading': [],\n",
    "            'content_mismatch': defaultdict(list),\n",
    "            'unlearned_chars': defaultdict(list),\n",
    "            'reappearance': defaultdict(lambda: {'count':0, 'intervals':[]})\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 预处理阶段：收集字符出现信息\n",
    "    # =================================================================\n",
    "    for lesson in lessons:\n",
    "        num = lesson['numero']\n",
    "        \n",
    "        # 记录shizi/xiezi出现情况\n",
    "        for char in lesson['shizi']:\n",
    "            analysis['chars']['shizi'][char].append(num)\n",
    "            if char not in analysis['chars']['first_shizi']:\n",
    "                analysis['chars']['first_shizi'][char] = num\n",
    "                \n",
    "        for char in lesson['xiezi']:\n",
    "            analysis['chars']['xiezi'][char].append(num)\n",
    "            if char not in analysis['chars']['first_xiezi']:\n",
    "                analysis['chars']['first_xiezi'][char] = num\n",
    "                \n",
    "        # 提取课文中的汉字（去除非汉字字符）\n",
    "        text = ''.join([\n",
    "            re.sub('？', '', re.sub(r'[^\\u4e00-\\u9fff]', '', line))\n",
    "            for part in lesson['content']\n",
    "            for line in (part if isinstance(part, list) else [part])\n",
    "        ])\n",
    "        analysis['chars']['text_chars'][num] = re.sub('？', '', re.sub(r'[^\\u4e00-\\u9fff]', '', lesson[\"title\"])) + \" \" + text\n",
    "    \n",
    "    # 问题检测逻辑\n",
    "    # =================================================================\n",
    "    # 检测1：重复的识字/写字任务\n",
    "    for char, nums in analysis['chars']['shizi'].items():\n",
    "        if len(nums) > 1:\n",
    "            analysis['issues']['duplicate_shizi'][char] = nums\n",
    "            \n",
    "    for char, nums in analysis['chars']['xiezi'].items():\n",
    "        if len(nums) > 1:\n",
    "            analysis['issues']['duplicate_xiezi'][char] = nums\n",
    "    \n",
    "    # 检测2：写字先于识字\n",
    "    for char, xiezi_first in analysis['chars']['first_xiezi'].items():\n",
    "        shizi_first = analysis['chars']['first_shizi'].get(char, float('inf'))\n",
    "        if shizi_first > xiezi_first:\n",
    "            analysis['issues']['writing_before_reading'].append({\n",
    "                'char': char,\n",
    "                'xiezi_first': xiezi_first,\n",
    "                'shizi_first': shizi_first if shizi_first != float('inf') else '从未出现'\n",
    "            })\n",
    "    \n",
    "    # 检测：识字后是否学习写字\n",
    "    for char, shizi_first in analysis['chars']['first_shizi'].items():\n",
    "        xiezi_first = analysis['chars']['first_xiezi'].get(char, float('inf'))\n",
    "        if xiezi_first >= shizi_first:\n",
    "            analysis['issues']['writing_after_reading'].append({\n",
    "                'char': char,\n",
    "                'xiezi_first': xiezi_first if xiezi_first != float('inf') else '从未出现',\n",
    "                'shizi_first': shizi_first,\n",
    "                \"lag\": xiezi_first - shizi_first if xiezi_first != float('inf') else -1\n",
    "            })\n",
    "\n",
    "    # 检测3：学习内容是否在课文中\n",
    "    for lesson in lessons:\n",
    "        num = lesson['numero']\n",
    "        text_chars = analysis['chars']['text_chars'][num]\n",
    "        \n",
    "        # 检查识字\n",
    "        for char in lesson['shizi']:\n",
    "            if char not in text_chars:\n",
    "                analysis['issues']['content_mismatch'][num].append(\n",
    "                    f\"识字 '{char}' 未在课文出现\"\n",
    "                )\n",
    "                \n",
    "        # 检查写字\n",
    "        for char in lesson['xiezi']:\n",
    "            if char not in text_chars:\n",
    "                analysis['issues']['content_mismatch'][num].append(\n",
    "                    f\"写字 '{char}' 未在课文出现\"\n",
    "                )\n",
    "                \n",
    "        # 检查词语（去除标点后检查）\n",
    "        for ci in lesson['ci']:\n",
    "            clean_ci = re.sub('？', '', re.sub(r'[^\\u4e00-\\u9fff]', '', ci))\n",
    "            if clean_ci not in ''.join(text_chars):\n",
    "                analysis['issues']['content_mismatch'][num].append(\n",
    "                    f\"词语 '{ci}' 未在课文出现\"\n",
    "                )\n",
    "    \n",
    "    # 检测4：未学先现\n",
    "    for num in analysis['chars']['text_chars']:\n",
    "        char_set = set(analysis['chars']['text_chars'][num])\n",
    "        for char in char_set:\n",
    "            if char != \" \":\n",
    "                # 该字在后续课程中才被列为shizi\n",
    "                first_shizi = analysis['chars']['first_shizi'].get(char, float('inf'))\n",
    "                if num < first_shizi:\n",
    "                    analysis['issues']['unlearned_chars'][num].append({\n",
    "                        'char': char,\n",
    "                        'first_shizi': first_shizi if first_shizi != float('inf') else '从未学习',\n",
    "                        \"lag\": first_shizi - num if first_shizi != float('inf') else -1\n",
    "                    })\n",
    "    \n",
    "    # 检测5：复现统计\n",
    "    shizi_order = sorted(analysis['chars']['first_shizi'].items(), key=lambda x: x[1])\n",
    "    for i, (char, first_num) in enumerate(shizi_order):\n",
    "        # 获取后续课程\n",
    "        subsequent = [l for l in lessons if l['numero'] > first_num]\n",
    "        prev = first_num\n",
    "        for lesson in subsequent:\n",
    "            if char in analysis['chars']['text_chars'][lesson['numero']]:\n",
    "                interval = lesson['numero'] - prev\n",
    "                prev = lesson['numero']\n",
    "                analysis['issues']['reappearance'][char]['count'] += 1\n",
    "                analysis['issues']['reappearance'][char]['intervals'].append(interval)\n",
    "    \n",
    "    # return analysis\n",
    "    # 生成报告\n",
    "    # =================================================================\n",
    "    brief_report = generate_brief_report(analysis, lessons)\n",
    "    full_report = generate_full_report(analysis, lessons)\n",
    "    \n",
    "    print(\"=== 简要检查报告 ===\")\n",
    "    print(brief_report)\n",
    "    \n",
    "    report_final_path = add_suffix(report_path, path)\n",
    "    Path(report_final_path).write_text(full_report, encoding='utf-8')\n",
    "\n",
    "    print(f\"\\n完整报告已保存至：{report_final_path}\")\n",
    "    return analysis\n",
    "\n",
    "def generate_brief_report(analysis, lessons):\n",
    "    \"\"\"生成命令行简要报告\"\"\"\n",
    "    report = []\n",
    "    \n",
    "    # 基础统计\n",
    "    report.append(f\"教材总课数：{len(lessons)}课\")\n",
    "    report.append(f\"总识字量：{len(analysis['chars']['shizi'])}字\")\n",
    "    report.append(f\"总写字量：{len(analysis['chars']['xiezi'])}字\")\n",
    "    \n",
    "    # 问题汇总\n",
    "    report.append(\"\\n=== 问题汇总 ===\")\n",
    "    report.append(f\"1. 重复识字任务：{len(analysis['issues']['duplicate_shizi'])}字\")\n",
    "    report.append(f\"2. 重复写字任务：{len(analysis['issues']['duplicate_xiezi'])}字\")\n",
    "    report.append(f\"3. 写字先于识字：{len(analysis['issues']['writing_before_reading'])}处\")\n",
    "    report.append(f\"4. 内容不匹配：{sum(len(v) for v in analysis['issues']['content_mismatch'].values())}处\")\n",
    "    report.append(f\"5. 未学先现字符：{sum(len(v) for v in analysis['issues']['unlearned_chars'].values())}次\")\n",
    "\n",
    "    # 识写距离\n",
    "    rw_lags = np.array([int(v[\"lag\"]) for v in analysis['issues']['writing_after_reading'] if v[\"lag\"] >= 0])\n",
    "    # print(\"rw_lags\", rw_lags)\n",
    "    lag_min = np.min(rw_lags)\n",
    "    lag_max = np.max(rw_lags)\n",
    "    lag_avg = np.mean(rw_lags[rw_lags>0])\n",
    "    lag_med = np.median(rw_lags[rw_lags>0])\n",
    "    \n",
    "    report.append(\"\\n=== 识写间隔统计 ===\")\n",
    "    report.append(f\"最小距离：{lag_min}课\")\n",
    "    report.append(f\"最大距离：{lag_max}课\")\n",
    "    report.append(f\"平均距离：{lag_avg:.2f}课\")\n",
    "    report.append(f\"中位距离：{lag_med:.2f}课\")\n",
    "    values, counts = np.unique(rw_lags, return_counts=True)\n",
    "    bar_values = np.sort(values)[::-1]\n",
    "    if len(bar_values) > 4:\n",
    "        bar_values = bar_values[4]\n",
    "    else:\n",
    "        bar_values = 1\n",
    "    frequency = dict(zip(values, counts))\n",
    "    for i in sorted(values):\n",
    "        if i >= bar_values:\n",
    "            report.append(f\"- 距离{i:3d}课：{frequency[i]}字。\")\n",
    "\n",
    "    # 预现统计\n",
    "    aprioris = [c[\"lag\"] for v in analysis['issues']['unlearned_chars'].values() for c in v if c[\"lag\"] > 0]\n",
    "    never_again = [c[\"lag\"] for v in analysis['issues']['unlearned_chars'].values() for c in v if c[\"lag\"] < 0]\n",
    "    report.append(\"\\n=== 预现统计 ===\")\n",
    "    report.append(f\"平均预现距离：{sum(aprioris)/len(aprioris):.2f}课\")\n",
    "    report.append(f\"未学字数：{len(never_again)}字\")\n",
    "    \n",
    "    # 复现统计\n",
    "    reappear_counts = [s['count'] for s in analysis['issues']['reappearance'].values()]\n",
    "    report.append(\"\\n=== 复现统计 ===\")\n",
    "    report.append(f\"平均复现次数：{sum(reappear_counts)/len(reappear_counts):.1f}次\")\n",
    "    report.append(f\"未复现字数：{len([c for c in reappear_counts if c ==0])}字\")\n",
    "    \n",
    "    return '\\n'.join(report)\n",
    "\n",
    "def generate_full_report(analysis, lessons):\n",
    "    \"\"\"生成完整详细报告\"\"\"\n",
    "    report = []\n",
    "    \n",
    "    # 头部信息\n",
    "    report.append(\"教材分析完整报告\\n\")\n",
    "    report.append(f\"分析课程范围：第{lessons[0]['numero']}课 - 第{lessons[-1]['numero']}课\")\n",
    "    report.append(\"-\"*50)\n",
    "    \n",
    "    # 详细问题列表\n",
    "    def format_issue_list(title, items, formatter):\n",
    "        if not items:\n",
    "            return []\n",
    "        output = [f\"\\n【{title}】\"]\n",
    "        for item in items:\n",
    "            output.append(formatter(item))\n",
    "        return output\n",
    "    \n",
    "    # 1. 重复识字\n",
    "    report.extend(format_issue_list(\n",
    "        \"重复识字任务\",\n",
    "        analysis['issues']['duplicate_shizi'].items(),\n",
    "        lambda x: f\"字 '{x[0]}' 在以下课程重复出现：{x[1]}\"\n",
    "    ))\n",
    "    \n",
    "    # 2. 重复写字\n",
    "    report.extend(format_issue_list(\n",
    "        \"重复写字任务\", \n",
    "        analysis['issues']['duplicate_xiezi'].items(),\n",
    "        lambda x: f\"字 '{x[0]}' 在以下课程重复出现：{x[1]}\"\n",
    "    ))\n",
    "    \n",
    "    # 3. 写字先于识字\n",
    "    report.extend(format_issue_list(\n",
    "        \"写字先于识字\",\n",
    "        analysis['issues']['writing_before_reading'],\n",
    "        lambda x: f\"字 '{x['char']}': 第{x['xiezi_first']}课要求写字，但第{x['shizi_first']}课才要求识字\"\n",
    "    ))\n",
    "    \n",
    "    # 4. 内容不匹配\n",
    "    report.append(\"\\n【课文内容匹配问题】\")\n",
    "    for num in sorted(analysis['issues']['content_mismatch']):\n",
    "        issues = analysis['issues']['content_mismatch'][num]\n",
    "        report.append(f\"第{num}课：\")\n",
    "        report.extend([f\"  - {issue}\" for issue in issues])\n",
    "\n",
    "    # 5. 先识后写\n",
    "    rw_lags = np.array([int(v[\"lag\"]) for v in analysis['issues']['writing_after_reading'] if v[\"lag\"] >= 0])\n",
    "    # print(\"rw_lags\", rw_lags)\n",
    "    values, counts = np.unique(rw_lags, return_counts=True)\n",
    "    # frequency = dict(zip(values, counts))\n",
    "    samples = {val: [] for val in values}\n",
    "\n",
    "    report.append(\"\\n【识写间隔】\")\n",
    "    for item in analysis['issues']['writing_after_reading']:\n",
    "        if \"lag\" in item and item[\"lag\"] > 0:\n",
    "            samples[item[\"lag\"]].append(item)\n",
    "    \n",
    "    for val, count in zip(values, counts):\n",
    "        report.append(f\"- 间隔{val:3d}课的生字（共{count:3d}个）：\")\n",
    "        for item in samples[val]:\n",
    "            report.append(f\"    - {item['char']}: 第{item['shizi_first']}课 → 第{item['xiezi_first']}课\")\n",
    "    \n",
    "    # 6. 未学先现\n",
    "    report.append(\"\\n【未学先现字符】\")\n",
    "    for num in sorted(analysis['issues']['unlearned_chars']):\n",
    "        items = analysis['issues']['unlearned_chars'][num]\n",
    "        for item in items:\n",
    "            if item[\"lag\"] < 0:\n",
    "                report.append(f\"第{num}课出现预学字：\")\n",
    "                break\n",
    "        for item in items:\n",
    "            if item[\"lag\"] < 0:\n",
    "                # report.append(f\"  - '{item['char']}'（首次学习于第{item['first_shizi']}课）\")\n",
    "            # else:\n",
    "                report.append(f\"  - '{item['char']}'{item['first_shizi']}\")\n",
    "    \n",
    "    # 7. 复现分析\n",
    "    report.append(\"\\n【生字复现分析】\")\n",
    "    report.append(\"评估标准：\")\n",
    "    report.append(\"  优秀：复现≥3次且间隔≤5课\")\n",
    "    report.append(\"  良好：复现≥2次且间隔≤8课\")\n",
    "    report.append(\"  需改进：未达上述标准\")\n",
    "    \n",
    "    reappear_stats = []\n",
    "    for char, stats in analysis['issues']['reappearance'].items():\n",
    "        avg_interval = sum(stats['intervals'])/len(stats['intervals']) if stats['intervals'] else 0\n",
    "        evaluation = \"优秀\" if stats['count']>=3 and max(stats['intervals']+[0])<=5 else \\\n",
    "                    \"良好\" if stats['count']>=2 and max(stats['intervals']+[0])<=8 else \"需改进\"\n",
    "        reappear_stats.append((\n",
    "            char,\n",
    "            stats['count'],\n",
    "            f\"{avg_interval:.1f}\" if stats['count'] else \"无复现\",\n",
    "            evaluation\n",
    "        ))\n",
    "    \n",
    "    # 按复现次数排序\n",
    "    reappear_stats.sort(key=lambda x: (-x[1], x[0]))\n",
    "    \n",
    "\n",
    "    report.append(\"\\n复现不达标：\")\n",
    "    report.append(\"汉字 | 复现次数 | 平均间隔|\")\n",
    "    report.append(\"----|--------|--------|\")\n",
    "    for item in reappear_stats:\n",
    "        if item[3] == \"需改进\":\n",
    "            report.append(f\"{item[0]} | {item[1]} | {item[2]} |\")\n",
    "\n",
    "    report.append(\"\\n复现情况详情：\")\n",
    "    report.append(\"汉字 | 复现次数 | 平均间隔 | 评估 |\")\n",
    "    report.append(\"----|--------|--------|---- |\")\n",
    "    for item in reappear_stats:\n",
    "        report.append(f\"{item[0]} | {item[1]} | {item[2]} | {item[3]} |\")\n",
    "    \n",
    "    return '\\n'.join(report)\n",
    "\n",
    "analysis = analyze_textbook(\"../src/小学/发蒙识字.json\", \"../out/report.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是中国人 4 ['中', '人', '文', '上']\n",
      "一二三四五 23 ['一', '二', '三', '四', '五', '金', '木', '水', '火', '土', '天', '地', '日', '月', '分', '见', '下', '今', '古']\n",
      "人 33 ['头', '面', '身', '手', '足', '口', '牙', '目', '耳', '心']\n",
      "田 43 ['山', '川', '风', '云', '雨', '田', '力', '禾', '苗', '实']\n",
      "比大小 53 ['六', '七', '八', '九', '十', '比', '大', '小', '多', '少']\n",
      "开门 61 ['有', '开', '门', '爸', '妈', '在', '个', '只']\n",
      "山村 73 ['石', '不', '青', '鸡', '犬', '牛', '马', '闻', '村', '肥', '路', '归']\n",
      "你我他 82 ['学', '生', '是', '你', '我', '他', '也', '们', '国']\n",
      "田鸟 92 ['黄', '里', '麦', '鸟', '来', '飞', '吹', '点', '去', '了']\n",
      "鸟鱼虫 100 ['鱼', '虫', '爪', '尾', '巴', '毛', '羽', '吃']\n",
      "山羊 108 ['走', '叫', '羊', '草', '前', '后', '谁', '的']\n",
      "方向 120 ['方', '早', '向', '太', '阳', '边', '左', '右', '东', '南', '西', '北']\n",
      "我有一个家 131 ['儿', '子', '男', '女', '哥', '弟', '爱', '姐', '妹', '和', '家']\n",
      "看地图 136 ['外', '出', '入', '看', '图']\n",
      "工农兵 151 ['工', '农', '兵', '士', '民', '量', '好', '习', '起', '团', '结', '保', '卫', '世', '界']\n",
      "张开口 157 ['张', '舌', '几', '颗', '唇', '间']\n",
      "这是什么 161 ['什', '么', '这', '那']\n",
      "你吃什么 168 ['兔', '猴', '猫', '捉', '桃', '花', '白']\n",
      "谁比我高 173 ['高', '屋', '到', '说', '狗']\n",
      "时间 182 ['季', '年', '时', '秒', '现', '还', '半', '没', '吗']\n",
      "问路 193 ['问', '先', '往', '公', '园', '直', '再', '就', '怎', '请', '谢']\n",
      "过马路 202 ['过', '跑', '玩', '行', '道', '线', '红', '绿', '灯']\n",
      "你叫什么名字 213 ['名', '字', '王', '李', '刘', '赵', '陈', '吴', '杨', '姓', '光']\n",
      "画 222 ['远', '近', '色', '声', '听', '惊', '春', '画', '无']\n",
      "下雨啦 231 ['要', '啦', '吧', '种', '果', '树', '发', '芽', '长']\n",
      "小猫读书 240 ['干', '把', '打', '读', '书', '页', '会', '脚', '合']\n",
      "今天天气好 248 ['气', '两', '朵', '背', '包', '照', '空', '当']\n",
      "车子吃什么油 253 ['油', '车', '电', '汽', '用']\n",
      "小舟 260 ['弯', '舟', '尖', '坐', '闪', '星', '蓝']\n",
      "我有一张床 268 ['床', '席', '被', '枕', '柜', '台', '衣', '服']\n",
      "我的笔盒 277 ['盒', '钢', '铅', '尺', '橡', '皮', '笔', '黑', '支']\n",
      "找朋友 285 ['找', '呀', '朋', '友', '笑', '敬', '礼', '握']\n",
      "你开心吗？ 293 ['跳', '摇', '拍', '叽', '咕', '觉', '得', '嘴']\n",
      "切西瓜 300 ['拿', '刀', '切', '瓜', '块', '片', '成']\n",
      "丢手绢 308 ['丢', '轻', '绢', '告', '诉', '住', '快', '放']\n",
      "秋天 316 ['凉', '群', '叶', '从', '排', '落', '啊', '雁']\n",
      "画彩虹 325 ['布', '彩', '虹', '想', '橙', '加', '紫', '颜', '料']\n",
      "风筝 334 ['筝', '骨', '竹', '做', '纸', '清', '它', '越', '送']\n",
      "大桥 342 ['桥', '座', '江', '轮', '船', '以', '自', '能']\n",
      "中秋 349 ['秋', '节', '饼', '巷', '圆', '最', '明']\n",
      "拍皮球 356 ['球', '数', '夏', '汗', '巾', '冰', '喝']\n",
      "你知不知道 367 ['知', '广', '深', '海', '香', '老', '虎', '肉', '答', '案', '然']\n",
      "雪地里的小画家 376 ['雪', '鸭', '步', '蛙', '睡', '洞', '为', '梅', '枫']\n",
      "二月二 389 ['龙', '户', '抬', '使', '耕', '雷', '万', '物', '细', '绵', '似', '美', '酒']\n",
      "菜市场 403 ['菜', '买', '市', '丝', '豆', '姜', '卜', '葱', '场', '萝', '茄', '柿', '兰', '扁']\n",
      "大扫除 419 ['扫', '抹', '动', '活', '始', '除', '板', '齐', '拖', '提', '桶', '亮', '堂', '窗', '净', '乐']\n",
      "东海龙宫 429 ['井', '乌', '流', '宫', '晶', '宝', '贝', '玉', '夜', '珠']\n",
      "江南 434 ['可', '采', '何', '莲', '戏']\n",
      "鹅 442 ['鹅', '曲', '项', '歌', '浮', '掌', '拨', '波']\n",
      "春天在哪里 452 ['哪', '翠', '眼', '睛', '鹂', '倒', '影', '湖', '映', '唱']\n",
      "看星星 463 ['横', '浅', '织', '郎', '勺', '像', '斗', '晚', '指', '着', '央']\n",
      "小燕子 477 ['燕', '穿', '啥', '厂', '更', '盖', '装', '新', '机', '器', '欢', '迎', '久', '丽']\n",
      "冬天的兴安岭 486 ['林', '河', '厚', '冬', '兴', '安', '灰', '望', '处']\n"
     ]
    }
   ],
   "source": [
    "lessons = load_textbook(path_sz)\n",
    "ziji = {}\n",
    "nz = 0\n",
    "zinumbs = []\n",
    "zicount = []\n",
    "for lesson in lessons:\n",
    "    zis = []\n",
    "    for zi in lesson['shizi']:\n",
    "        if zi not in ziji:\n",
    "            ziji[zi] = 0\n",
    "            zis.append(zi)\n",
    "    n = len(zis)\n",
    "    nz += n\n",
    "    zinumbs.append(n)\n",
    "    zicount.append(nz)\n",
    "    print(lesson['title'], nz, zis)\n",
    "\n",
    "zinumbs = np.array(zinumbs)\n",
    "zicount = np.array(zicount)\n",
    "# print(zinumbs)\n",
    "# print(zicount)\n",
    "# plt.plot(zinumbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 1 total 5 rest 0\n",
      "level 2 total 12 rest 0\n",
      "level 3 total 24 rest 1\n",
      "['她']\n",
      "level 4 total 38 rest 10\n",
      "['而', '对', '于', '之', '都', '如', '事', '第', '样', '作']\n",
      "level 5 total 61 rest 30\n",
      "['总', '情', '己', '但', '些', '所', '同', '又', '意', '期']\n",
      "['经', '回', '位', '因', '很', '给', '法', '斯', '次', '者']\n",
      "['已', '亲', '其', '进', '此', '话', '常', '与', '正', '感']\n",
      "level 6 total 92 rest 40\n",
      "['理', '尔', '定', '本', '特', '孩', '相', '将', '全', '信']\n",
      "['重', '每', '并', '别', '真', '才', '便', '夫', '部', '等']\n",
      "['体', '却', '主', '利', '受', '表', '德', '克', '代', '员']\n",
      "['许', '零', '由', '死', '写', '性', '或', '难', '教', '命']\n",
      "level 7 total 148 rest 79\n",
      "['拉', '神', '记', '让', '母', '父', '应', '平', '报', '关']\n",
      "['至', '认', '接', '内', '英', '军', '候', '岁', '度', '带']\n",
      "['解', '任', '原', '变', '通', '师', '立', '象', '失', '满']\n",
      "['战', '格', '音', '条', '呢', '病', '达', '完', '求', '化']\n",
      "['业', '思', '非', '罗', '钱', '积', '语', '元', '喜', '曾']\n",
      "['离', '科', '言', '约', '各', '即', '反', '题', '必', '该']\n",
      "['论', '交', '终', '医', '制', '决', '传', '运', '及', '则']\n",
      "['房', '院', '苦', '品', '产', '精', '视', '连', '司']\n"
     ]
    }
   ],
   "source": [
    "s7 = load_cn_json(\"simple700.json\")\n",
    "f9 = load_cn_json(\"frequent1000.json\")\n",
    "\n",
    "nin = []\n",
    "n = 0\n",
    "for lv in f9[:7]:\n",
    "    ni = []\n",
    "    n += 1\n",
    "    for zi in lv:\n",
    "        if zi in ziji:\n",
    "            pass\n",
    "        else:\n",
    "            ni.append(zi)\n",
    "    nin.append(ni)\n",
    "    print(\"level\", n, \"total\", len(lv), \"rest\", len(ni))\n",
    "    if ni:\n",
    "        nice_print(ni, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = {}\n",
    "packages[\"ctex\"] = []\n",
    "packages[\"titlesec\"] = []\n",
    "packages[\"xeCJK\"] = []\n",
    "packages[\"verse\"] = []\n",
    "packages[\"fontspec,xunicode,xltxtra\"] = []\n",
    "packages[\"xpinyin\"] = pinyin\n",
    "packages[\"hanzibox\"] = hanzibox\n",
    "packages[\"geometry\"] = geometry\n",
    "packages[\"indentfirst\"] = []\n",
    "packages[\"pifont\"] = []\n",
    "packages[\"enumitem\"] = []\n",
    "packages[\"footmisc\"] = {\"declarations\": [\"perpage\", \"symbol*\"]}\n",
    "xcolor = {}\n",
    "xcolor[\"declarations\"] = [\"table\", \"dvipsnames\"]\n",
    "packages[\"xcolor\"] = xcolor\n",
    "\n",
    "typesettings = {}\n",
    "typesettings[\"vspaces\"] = {\"after_title\": 36, \"after_author\": 16, \"after_content\": 16}\n",
    "typesettings[\"font\"] = {\"plaintext\": {\"size\": \"large\"}}\n",
    "\n",
    "booktitle = \"发蒙识字\"\n",
    "texts_sz = load_cn_json(\"../src/小学/发蒙识字.json\")\n",
    "\n",
    "lineskip = \"24pt\"\n",
    "parskip = \"6pt\"\n",
    "package_update_xcolor(packages, texts_sz)\n",
    "header, footer = make_ctex_env(packages=packages, title=booktitle, parskip=parskip, lineskip=lineskip)\n",
    "with open(\"发蒙识字.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(header + \"\\n\")\n",
    "    for title, text in sort_dict_with(texts_sz, key=\"numero\"):\n",
    "        # print(title)\n",
    "        f.write(text_to_tex_str(text, typesettings=typesettings) + \"\\n\")\n",
    "        # break\n",
    "    f.write(footer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"一、二、三、十、木、禾、上、下、土、个、八、入、大、天、人、火、文、六、七、儿、九、无、口、日、中、了、子、门、月、不、开、四、五、目、耳、头、米、见、白、田、电、也、长、山、出、飞、马、鸟、云、公、车、牛、羊、小、少、巾、牙、尺、毛、卜、又、心、风、力、手、水、广、升、足、走、方、半、巴、业、本、平、书、自、已、东、西、回、片、皮、生、里、果、几、用、鱼、今、正、雨、两、瓜、衣、来、年、左、右、万、百、丁、齐、冬、说、友、话、春、朋、高、你、绿、们、花、红、草、爷、亲、节、的、岁、行、古、处、声、知、多、忙、洗、真、认、父、扫、母、爸、写、全、完、关、家、看、笑、着、兴、画、会、妈、合、奶、放、午、收、女、气、太、早、去、亮、和、李、语、秀、千、香、听、远、唱、定、连、向、以、更、后、意、主、总、先、起、干、明、赶、净、同、专、工、才、级、队、蚂、蚁、前、房、空、网、诗、黄、林、闭、童、立、是、我、朵、叶、美、机、她、过、他、时、送、让、吗、往、吧、得、虫、很、河、借、姐、呢、呀、哪、谁、凉、怕、量、跟、最、园、脸、因、阳、为、光、可、法、石、找、办、许、别、那、到、都、吓、叫、再、做、象、点、像、照、沙、海、桥、军、竹、苗、井、面、乡、忘、想、念、王、这、从、进、边、道、贝、男、原、爱、虾、跑、吹、乐、地、老、快、师、短、淡、对、热、冷、情、拉、活、把、种、给、吃、练、学、习、非、苦、常、问、伴、间、共、伙、汽、分、要、没、孩、位、选、北、湖、南、秋、江、只、帮、星、请、雪、就、球、跳、玩、桃、树、刚、兰、座、各、带、坐、急、名、发、成、动、晚、新、有、么、在、变、什、条\"\n",
    "len(s.split(\"、\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.zhonghuadiancang.com/wenxueyishu/liyoucaibanhua/150596.html\"\n",
    "url = \"https://www.zhonghuadiancang.com/wenxueyishu/liyoucaibanhua/150597.html\"\n",
    "url = \"https://www.zhonghuadiancang.com/wenxueyishu/liyoucaibanhua/150598.html\"\n",
    "url = \"https://www.99csw.com/book/2780/86125.htm\"\n",
    "\n",
    "res = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.content.decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF转图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../语文/洱海一枝春/洱海一枝春_1.png\n",
      "Saved ../语文/洱海一枝春/洱海一枝春_2.png\n",
      "Saved ../语文/洱海一枝春/洱海一枝春_3.png\n",
      "Saved ../语文/洱海一枝春/洱海一枝春_4.png\n",
      "Saved ../语文/洱海一枝春/洱海一枝春_5.png\n",
      "Saved ../语文/洱海一枝春/洱海一枝春_6.png\n",
      "Saved ../语文/洱海一枝春/洱海一枝春_7.png\n"
     ]
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "\n",
    "# Function to convert PDF to images\n",
    "def pdf_to_images(pdf_path, output_folder, filename):\n",
    "    # Convert PDF to list of PIL.Image\n",
    "    images = convert_from_path(pdf_path)\n",
    "\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Save each image in the list\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = os.path.join(output_folder, f\"{filename}_{i + 1}.png\")  # Adjust extension as needed\n",
    "        image.save(image_path, \"PNG\")  # Adjust format as needed\n",
    "        print(f\"Saved {image_path}\")\n",
    "\n",
    "# Example usage\n",
    "title = r\"洱海一枝春\"\n",
    "pdf_path = r\"../语文/单篇课文.pdf\"  # Replace with your PDF file path\n",
    "output_folder = f\"../语文/{title}/\"  # Replace with desired output folder path\n",
    "pdf_to_images(pdf_path, output_folder, title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hanzi2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
